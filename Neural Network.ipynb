{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#parse data \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#label encoding on categorical data \n",
    "\n",
    "#FAMA 49CRSP Common Stocks \n",
    "df = pd.read_csv('ee6d2f60cdafb550.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 25 25 ... 27 45 47]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-0f756f0b5870>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minteger_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFFI49_desc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minteger_encoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivyield_Median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivyield_Median\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-0f756f0b5870>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minteger_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFFI49_desc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minteger_encoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivyield_Median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivyield_Median\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing \n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#preprocessing here\n",
    "#sort by date \n",
    "df = df.sort_values(by = 'public_date', ascending = True)\n",
    "\n",
    "#encode integer categories into numbers \n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df.FFI49_desc)\n",
    "print(integer_encoded)\n",
    "df.FFI49_desc = integer_encoded\n",
    "df.divyield_Median = [float(x.strip('%'))/100 for x in df.divyield_Median]\n",
    "print(df)\n",
    "\n",
    "#todo: https://www.gsb.stanford.edu/library/articles/databases/links/financial-ratios-suite?fbclid=IwAR0EGNGk9DdxQjEHfdaoUhdY3tNzAWDogYDzuuJi1zT_muL-uJtWQw19Fzk\n",
    "\n",
    "#get output first \n",
    "ewlabels = df.indret_ew\n",
    "vwlabels = df.indret_vw\n",
    "\n",
    "#3year on year change as a prediction feature, raw pct change \n",
    "yoythree = ewlabels.diff(periods = 3)\n",
    "#3 years rolling percent change, averaged ie. (y1-y2 + (y3-y2)change)/2 \n",
    "rollavgpct = ewlabels.rolling(3).mean()\n",
    "\n",
    "#drop first 3 years\n",
    "df = df.iloc[3:]\n",
    "ewlabels = ewlabels.iloc[3:]\n",
    "yoythree = yoythree.iloc[3:]\n",
    "#yoypctthree = yoypctthree.iloc[3:]\n",
    "rollavgpct = rollavgpct.iloc[3:]\n",
    "\n",
    "#add -1 and 1 so the bins will take on bins to be equal and set to max -1 and 1\n",
    "extrema = pd.Series([-1,1])\n",
    "ewnlabels = ewlabels.append(extrema)\n",
    "\n",
    "#make a new output (bucket by percentage?)\n",
    "enc = KBinsDiscretizer(n_bins=8, encode='ordinal',strategy = 'uniform')\n",
    "ewnlabels = np.asarray(ewnlabels)\n",
    "ewnlabels = ewnlabels.reshape((-1,1))\n",
    "labels_binned = enc.fit_transform(ewnlabels)\n",
    "\n",
    "labels_binned = labels_binned[:-2]\n",
    "\n",
    "#1 Split-Timer series data, 0.64 Train, 0.16 dev, 0.2 Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, labels_binned, test_size = 0.2, shuffle = False)\n",
    "x_tra, x_dev, y_tra, y_dev = train_test_split(x_train, y_train, test_size = 0.2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       public_date  FFI49_desc  NFIRM  indret_ew  indret_vw  dpr_Median  \\\n",
      "12477     19931130          25      5  -0.022123  -0.014000       0.479   \n",
      "12524     19931231          25      5   0.096039   0.072342       0.479   \n",
      "12571     19940131          25      5   0.043779   0.009888       0.479   \n",
      "12618     19940228          25      6   0.012589   0.039537       0.394   \n",
      "12665     19940331          25      6  -0.012399  -0.052069       0.394   \n",
      "12712     19940430          25      6  -0.026779  -0.024747       0.394   \n",
      "12759     19940531          25      6  -0.001201   0.023906       0.394   \n",
      "12806     19940630          25      6  -0.001294  -0.037881       0.394   \n",
      "12853     19940731          25      6   0.023423   0.043118       0.394   \n",
      "12900     19940831          25      6   0.066949   0.055253       0.398   \n",
      "12947     19940930          25      6   0.004471  -0.026728       0.398   \n",
      "12994     19941031          25      6  -0.044276  -0.015566       0.398   \n",
      "13041     19941130          25      6  -0.040811  -0.035578       0.390   \n",
      "13088     19941231          25      6   0.007111   0.043045       0.390   \n",
      "13135     19950131          25      6   0.021079   0.023549       0.390   \n",
      "13182     19950228          25      6   0.058941   0.058359       0.422   \n",
      "13229     19950331          25      6   0.057380   0.023958       0.422   \n",
      "13276     19950430          25      6   0.062681   0.029973       0.422   \n",
      "13323     19950531          25      6   0.037249   0.031139       0.378   \n",
      "13370     19950630          25      6   0.056587   0.052070       0.378   \n",
      "13417     19950731          25      6   0.004115  -0.005009       0.378   \n",
      "13464     19950831          25      6  -0.000317   0.011758       0.422   \n",
      "13511     19950930          25      6   0.053875   0.015358       0.422   \n",
      "13558     19951031          25      6   0.004551  -0.003330       0.422   \n",
      "13605     19951130          25      6   0.047691   0.086727       0.409   \n",
      "13652     19951231          25      6   0.026102   0.035340       0.409   \n",
      "13699     19960131          25      6   0.044772   0.033413       0.409   \n",
      "13745     19960229          25      6   0.003002  -0.025817       0.382   \n",
      "13791     19960331          25      6  -0.020940   0.020593       0.382   \n",
      "13837     19960430          25      6   0.057884   0.024875       0.382   \n",
      "...            ...         ...    ...        ...        ...         ...   \n",
      "23058     20130331          23     10   0.032211   0.020075       0.394   \n",
      "23056     20130331          21     10   0.040985   0.041604       0.029   \n",
      "23035     20130331           0      5   0.049727   0.057079       0.267   \n",
      "23036     20130331           1      1   0.045531   0.045531       0.304   \n",
      "23037     20130331           2      5   0.031169   0.042730       0.051   \n",
      "23038     20130331           3     28   0.049929   0.044499       0.232   \n",
      "23039     20130331           4      5   0.072159   0.051390       0.526   \n",
      "23040     20130331           5      4   0.029491   0.027265       0.266   \n",
      "23041     20130331           6      1   0.099651   0.099651       0.437   \n",
      "23042     20130331           7      2   0.058239   0.061218       0.074   \n",
      "23043     20130331           8     16   0.054364   0.054677       0.315   \n",
      "23044     20130331           9     11   0.005962   0.008999       0.380   \n",
      "23057     20130331          22      4   0.057587   0.029427       0.000   \n",
      "23046     20130331          11      6   0.005031   0.033397       0.262   \n",
      "23047     20130331          12      6   0.074877   0.074545       0.023   \n",
      "23049     20130331          14     21   0.074698   0.076043       0.055   \n",
      "23050     20130331          15      2  -0.029402  -0.021600       0.463   \n",
      "23051     20130331          16     14   0.049968   0.033095       0.349   \n",
      "23052     20130331          17     16   0.066768   0.068974       0.421   \n",
      "23053     20130331          18      3   0.039063   0.040320       0.260   \n",
      "23054     20130331          19      1   0.050261   0.050261       0.369   \n",
      "23055     20130331          20      1   0.096818   0.096818       0.497   \n",
      "23045     20130331          10     25   0.031682   0.020333       0.284   \n",
      "23113     20130430          32      8  -0.017928   0.000664       0.650   \n",
      "23112     20130430          31      4   0.050958   0.049084       0.581   \n",
      "23111     20130430          30     27  -0.029983  -0.008038       0.170   \n",
      "23106     20130430          25      9  -0.013433  -0.010253       0.126   \n",
      "23109     20130430          28     12  -0.032450  -0.018526       0.124   \n",
      "23108     20130430          27      7   0.028532   0.024951       0.378   \n",
      "23107     20130430          26     15  -0.013529  -0.014217       0.223   \n",
      "\n",
      "       PEG_trailing_Median  bm_Median  CAPEI_Median  divyield_Median  \\\n",
      "12477                3.865      0.401        19.861          0.02730   \n",
      "12524                4.122      0.401        21.179          0.02550   \n",
      "12571                4.190      0.401        21.483          0.02120   \n",
      "12618               11.991      0.406        22.148          0.02340   \n",
      "12665               11.971      0.406        20.340          0.02330   \n",
      "12712               11.623      0.406        19.663          0.02510   \n",
      "12759                3.700      0.393        19.092          0.02450   \n",
      "12806                3.833      0.393        18.900          0.02520   \n",
      "12853                3.825      0.393        19.442          0.02480   \n",
      "12900                1.836      0.397        19.539          0.02390   \n",
      "12947                1.765      0.397        18.768          0.02390   \n",
      "12994                1.664      0.397        18.101          0.02440   \n",
      "13041                1.371      0.382        17.360          0.02690   \n",
      "13088                1.447      0.382        18.339          0.02710   \n",
      "13135                1.376      0.382        18.007          0.02600   \n",
      "13182                1.572      0.390        19.144          0.02490   \n",
      "13229                1.628      0.390        19.835          0.02460   \n",
      "13276                1.653      0.395        25.704          0.02370   \n",
      "13323                1.430      0.359        26.911          0.02230   \n",
      "13370                1.520      0.359        27.939          0.02120   \n",
      "13417                1.564      0.359        28.178          0.02170   \n",
      "13464                1.358      0.334        27.136          0.02140   \n",
      "13511                1.124      0.334        27.864          0.02120   \n",
      "13558                1.254      0.334        27.607          0.02160   \n",
      "13605                1.052      0.348        28.600          0.02040   \n",
      "13652                1.076      0.348        30.916          0.01970   \n",
      "13699                1.046      0.339        31.812          0.01740   \n",
      "13745                0.043      0.305        28.287          0.01720   \n",
      "13791                0.050      0.305        26.299          0.01860   \n",
      "13837                0.051      0.305        28.086          0.01750   \n",
      "...                    ...        ...           ...              ...   \n",
      "23058                1.474      0.234        24.443          0.01690   \n",
      "23056                0.585      0.497        13.392          0.02230   \n",
      "23035                1.793      0.344        21.367          0.01900   \n",
      "23036                1.238      0.257        32.323          0.01420   \n",
      "23037                0.110      0.402        12.823          0.03040   \n",
      "23038                0.326      1.030        16.835          0.01950   \n",
      "23039                7.647      0.515        20.569          0.02020   \n",
      "23040                0.178      0.344        -0.426          0.01860   \n",
      "23041                1.191      0.567        -5.182          0.03660   \n",
      "23042                1.756      0.240        39.988          0.01090   \n",
      "23043                2.523      0.332        23.321          0.02190   \n",
      "23044                0.551      0.254        24.137          0.01760   \n",
      "23057                2.103      0.360        15.202          0.02130   \n",
      "23046                0.563      0.304        21.120          0.01440   \n",
      "23047                0.668      0.519         4.381          0.00617   \n",
      "23049                1.931      0.274        24.185          0.02990   \n",
      "23050                1.228      0.229        20.840          0.02560   \n",
      "23051                0.798      1.077        19.258          0.02240   \n",
      "23052                1.985      0.264        22.271          0.02100   \n",
      "23053                0.477      0.240        41.921          0.05560   \n",
      "23054                0.122      0.634        15.085          0.04060   \n",
      "23055                4.268      0.001        10.864          0.04770   \n",
      "23045                0.638      0.373        16.341          0.02390   \n",
      "23113                1.567      0.533        23.017          0.02610   \n",
      "23112                2.907      0.629        32.220          0.02760   \n",
      "23111                0.411      0.838        15.139          0.01450   \n",
      "23106                1.077      0.424        28.076          0.00914   \n",
      "23109                2.191      0.403        18.761          0.01330   \n",
      "23108                1.323      0.165        40.890          0.01950   \n",
      "23107                0.514      0.404        17.645          0.01950   \n",
      "\n",
      "               ...            rect_turn_Median  sale_equity_Median  \\\n",
      "12477          ...                       5.364               2.755   \n",
      "12524          ...                       5.364               2.755   \n",
      "12571          ...                       5.319               2.795   \n",
      "12618          ...                       4.789               2.451   \n",
      "12665          ...                       4.789               2.451   \n",
      "12712          ...                       4.789               2.525   \n",
      "12759          ...                       4.704               2.516   \n",
      "12806          ...                       4.704               2.516   \n",
      "12853          ...                       4.704               2.448   \n",
      "12900          ...                       4.589               2.449   \n",
      "12947          ...                       4.589               2.449   \n",
      "12994          ...                       4.589               2.513   \n",
      "13041          ...                       4.454               2.458   \n",
      "13088          ...                       4.454               2.458   \n",
      "13135          ...                       4.454               2.435   \n",
      "13182          ...                       4.443               2.568   \n",
      "13229          ...                       4.443               2.568   \n",
      "13276          ...                       4.443               2.526   \n",
      "13323          ...                       4.521               2.463   \n",
      "13370          ...                       4.521               2.463   \n",
      "13417          ...                       4.521               2.282   \n",
      "13464          ...                       4.551               2.364   \n",
      "13511          ...                       4.551               2.364   \n",
      "13558          ...                       4.551               2.510   \n",
      "13605          ...                       4.621               2.689   \n",
      "13652          ...                       4.621               2.689   \n",
      "13699          ...                       4.621               2.692   \n",
      "13745          ...                       4.713               2.683   \n",
      "13791          ...                       4.713               2.683   \n",
      "13837          ...                       4.713               2.673   \n",
      "...            ...                         ...                 ...   \n",
      "23058          ...                       8.494               2.594   \n",
      "23056          ...                       6.344               1.512   \n",
      "23035          ...                       5.591               3.826   \n",
      "23036          ...                       4.380               1.160   \n",
      "23037          ...                       6.165               2.916   \n",
      "23038          ...                       0.102               0.511   \n",
      "23039          ...                       5.524               1.052   \n",
      "23040          ...                       7.461               2.117   \n",
      "23041          ...                       7.596               2.277   \n",
      "23042          ...                       8.039               7.892   \n",
      "23043          ...                       6.061               1.728   \n",
      "23044          ...                       6.186               2.532   \n",
      "23057          ...                       7.021               2.131   \n",
      "23046          ...                      11.197               2.036   \n",
      "23047          ...                      12.079               1.887   \n",
      "23049          ...                       5.652               1.037   \n",
      "23050          ...                       5.398               2.794   \n",
      "23051          ...                       5.195               0.533   \n",
      "23052          ...                      13.348               4.549   \n",
      "23053          ...                      12.919               3.300   \n",
      "23054          ...                      12.320               0.716   \n",
      "23055          ...                       7.053            1209.790   \n",
      "23045          ...                       7.953               1.158   \n",
      "23113          ...                       7.416               3.696   \n",
      "23112          ...                       6.897               1.694   \n",
      "23111          ...                       6.841               0.862   \n",
      "23106          ...                       5.435               0.960   \n",
      "23109          ...                       5.786               1.170   \n",
      "23108          ...                      29.925               2.409   \n",
      "23107          ...                       4.949               1.697   \n",
      "\n",
      "       sale_invcap_Median  sale_nwc_Median  accrual_Median  rd_sale_Median  \\\n",
      "12477               2.345            8.303           0.057           0.080   \n",
      "12524               2.345            8.303           0.057           0.080   \n",
      "12571               2.347            8.303           0.057           0.080   \n",
      "12618               2.121            7.341           0.038           0.068   \n",
      "12665               2.121            7.341           0.038           0.068   \n",
      "12712               2.163            7.321           0.038           0.068   \n",
      "12759               2.165            7.181           0.029           0.066   \n",
      "12806               2.165            7.181           0.029           0.066   \n",
      "12853               2.098            6.644           0.018           0.066   \n",
      "12900               2.110            6.244           0.001           0.064   \n",
      "12947               2.110            6.244           0.001           0.064   \n",
      "12994               2.162            6.483          -0.012           0.064   \n",
      "13041               2.132            6.226           0.012           0.063   \n",
      "13088               2.132            6.226           0.012           0.063   \n",
      "13135               2.121            6.171           0.012           0.063   \n",
      "13182               2.167            5.887           0.018           0.061   \n",
      "13229               2.167            5.887           0.018           0.061   \n",
      "13276               2.141            5.817           0.018           0.061   \n",
      "13323               2.157            5.438           0.016           0.059   \n",
      "13370               2.157            5.438           0.016           0.059   \n",
      "13417               2.032            5.562           0.016           0.059   \n",
      "13464               2.043            5.071           0.012           0.058   \n",
      "13511               2.043            5.071           0.012           0.058   \n",
      "13558               2.141            5.032           0.012           0.058   \n",
      "13605               2.123            5.813           0.007           0.056   \n",
      "13652               2.123            5.813           0.007           0.056   \n",
      "13699               2.116            5.819           0.007           0.056   \n",
      "13745               2.135            6.049           0.022           0.055   \n",
      "13791               2.135            6.049           0.022           0.055   \n",
      "13837               2.118            6.007           0.022           0.055   \n",
      "...                   ...              ...             ...             ...   \n",
      "23058               2.254            6.278           0.041           0.015   \n",
      "23056               1.279            3.472           0.070           0.119   \n",
      "23035               2.108            5.273           0.024           0.041   \n",
      "23036               0.995            2.585           0.061           0.108   \n",
      "23037               1.809            7.682           0.048           0.018   \n",
      "23038               0.337            2.075           0.008           0.000   \n",
      "23039               0.456            2.018           0.016           0.000   \n",
      "23040               1.518            5.716           0.020           0.009   \n",
      "23041               1.344           38.734           0.051           0.000   \n",
      "23042               1.775           13.884           0.052           0.006   \n",
      "23043               1.233            7.192           0.040           0.000   \n",
      "23044               1.322            4.658           0.045           0.024   \n",
      "23057               1.067           12.157           0.045           0.000   \n",
      "23046               1.725            4.386           0.036           0.000   \n",
      "23047               1.294            6.316          -0.018           0.000   \n",
      "23049               0.782            2.768           0.040           0.167   \n",
      "23050               1.959            6.172           0.058           0.032   \n",
      "23051               0.407            2.322           0.005           0.000   \n",
      "23052               1.718           10.658           0.033           0.006   \n",
      "23053               0.876            4.813           0.050           0.091   \n",
      "23054               0.425            3.519           0.017           0.000   \n",
      "23055               7.614           27.754          -0.031           0.013   \n",
      "23045               0.867            2.360           0.052           0.124   \n",
      "23113               1.692            7.864           0.063           0.015   \n",
      "23112               0.684           70.480           0.056           0.000   \n",
      "23111               0.597           16.375           0.093           0.000   \n",
      "23106               0.749            4.563           0.029           0.062   \n",
      "23109               0.833            2.176           0.041           0.067   \n",
      "23108               2.186            7.592           0.078           0.000   \n",
      "23107               1.182            3.761          -0.005           0.023   \n",
      "\n",
      "       adv_sale_Median  staff_sale_Median  PEG_1yrforward_Median  \\\n",
      "12477            0.016              0.000                  0.677   \n",
      "12524            0.016              0.000                  0.722   \n",
      "12571            0.016              0.000                  0.814   \n",
      "12618            0.011              0.000                  1.108   \n",
      "12665            0.011              0.000                  1.013   \n",
      "12712            0.011              0.000                  0.897   \n",
      "12759            0.011              0.000                  0.816   \n",
      "12806            0.011              0.000                  0.866   \n",
      "12853            0.011              0.000                  1.522   \n",
      "12900            0.012              0.000                  1.517   \n",
      "12947            0.012              0.000                  1.456   \n",
      "12994            0.012              0.000                  0.806   \n",
      "13041            0.008              0.000                  0.923   \n",
      "13088            0.008              0.000                  0.975   \n",
      "13135            0.008              0.000                  1.063   \n",
      "13182            0.000              0.000                  1.046   \n",
      "13229            0.000              0.000                  1.123   \n",
      "13276            0.000              0.000                  1.136   \n",
      "13323            0.000              0.000                  1.055   \n",
      "13370            0.000              0.000                  1.134   \n",
      "13417            0.000              0.000                  1.098   \n",
      "13464            0.000              0.000                  1.022   \n",
      "13511            0.000              0.000                  1.002   \n",
      "13558            0.000              0.000                  0.935   \n",
      "13605            0.000              0.000                  0.873   \n",
      "13652            0.000              0.000                  0.863   \n",
      "13699            0.000              0.000                  0.840   \n",
      "13745            0.000              0.000                  0.906   \n",
      "13791            0.000              0.000                  1.060   \n",
      "13837            0.000              0.000                  1.169   \n",
      "...                ...                ...                    ...   \n",
      "23058            0.042              0.000                  1.590   \n",
      "23056            0.003              0.000                 -0.242   \n",
      "23035            0.000              0.000                  1.246   \n",
      "23036            0.006              0.000                  1.077   \n",
      "23037            0.000              0.000                  2.324   \n",
      "23038            0.015              0.288                  1.134   \n",
      "23039            0.108              0.000                  2.673   \n",
      "23040            0.000              0.000                 -0.182   \n",
      "23041            0.000              0.000                 -2.847   \n",
      "23042            0.000              0.000                  2.859   \n",
      "23043            0.000              0.000                  1.421   \n",
      "23044            0.000              0.000                  1.365   \n",
      "23057            0.000              0.000                  2.925   \n",
      "23046            0.042              0.000                  1.507   \n",
      "23047            0.003              0.000                  0.211   \n",
      "23049            0.000              0.000                  2.017   \n",
      "23050            0.000              0.000                  2.977   \n",
      "23051            0.000              0.181                  1.144   \n",
      "23052            0.031              0.000                  2.254   \n",
      "23053            0.016              0.000                  1.906   \n",
      "23054            0.000              0.000                  1.003   \n",
      "23055            0.000              0.000                  2.465   \n",
      "23045            0.000              0.000                 -0.923   \n",
      "23113            0.000              0.000                  1.102   \n",
      "23112            0.000              0.000                  3.467   \n",
      "23111            0.000              0.000                  0.625   \n",
      "23106            0.001              0.000                  1.941   \n",
      "23109            0.000              0.000                  1.928   \n",
      "23108            0.024              0.000                  1.485   \n",
      "23107            0.000              0.000                  1.257   \n",
      "\n",
      "       PEG_ltgforward_Median  \n",
      "12477                  1.848  \n",
      "12524                  1.962  \n",
      "12571                  1.930  \n",
      "12618                  1.721  \n",
      "12665                  1.587  \n",
      "12712                  1.557  \n",
      "12759                  1.410  \n",
      "12806                  1.438  \n",
      "12853                  1.525  \n",
      "12900                  1.538  \n",
      "12947                  1.524  \n",
      "12994                  1.418  \n",
      "13041                  1.333  \n",
      "13088                  1.429  \n",
      "13135                  1.478  \n",
      "13182                  1.517  \n",
      "13229                  1.610  \n",
      "13276                  1.671  \n",
      "13323                  1.668  \n",
      "13370                  1.722  \n",
      "13417                  1.679  \n",
      "13464                  1.563  \n",
      "13511                  1.687  \n",
      "13558                  1.653  \n",
      "13605                  1.633  \n",
      "13652                  1.570  \n",
      "13699                  1.565  \n",
      "13745                  1.343  \n",
      "13791                  1.080  \n",
      "13837                  1.231  \n",
      "...                      ...  \n",
      "23058                  1.608  \n",
      "23056                  1.449  \n",
      "23035                  1.425  \n",
      "23036                  2.286  \n",
      "23037                  0.879  \n",
      "23038                  1.585  \n",
      "23039                  2.184  \n",
      "23040                  1.927  \n",
      "23041                  2.468  \n",
      "23042                  2.171  \n",
      "23043                  1.776  \n",
      "23044                  2.305  \n",
      "23057                  1.519  \n",
      "23046                  1.554  \n",
      "23047                  0.902  \n",
      "23049                  1.979  \n",
      "23050                  1.924  \n",
      "23051                  1.449  \n",
      "23052                  2.438  \n",
      "23053                 13.715  \n",
      "23054                 -6.157  \n",
      "23055                  1.354  \n",
      "23045                  1.523  \n",
      "23113                  2.010  \n",
      "23112                  3.428  \n",
      "23111                  2.009  \n",
      "23106                  1.840  \n",
      "23109                  1.753  \n",
      "23108                  2.175  \n",
      "23107                  1.283  \n",
      "\n",
      "[7435 rows x 76 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, input_dim=76, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\carol\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\carol\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "7435/7435 [==============================] - 1s 139us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 2/25\n",
      "7435/7435 [==============================] - 0s 63us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 3/25\n",
      "7435/7435 [==============================] - 0s 46us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 4/25\n",
      "7435/7435 [==============================] - 0s 50us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 5/25\n",
      "7435/7435 [==============================] - 0s 48us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 6/25\n",
      "7435/7435 [==============================] - 0s 50us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 7/25\n",
      "7435/7435 [==============================] - 0s 48us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 8/25\n",
      "7435/7435 [==============================] - 0s 56us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 9/25\n",
      "7435/7435 [==============================] - 0s 53us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 10/25\n",
      "7435/7435 [==============================] - 0s 58us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 11/25\n",
      "7435/7435 [==============================] - 0s 46us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 12/25\n",
      "7435/7435 [==============================] - 0s 47us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 13/25\n",
      "7435/7435 [==============================] - 0s 46us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 14/25\n",
      "7435/7435 [==============================] - 0s 47us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 15/25\n",
      "7435/7435 [==============================] - 0s 46us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 16/25\n",
      "7435/7435 [==============================] - 0s 63us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 17/25\n",
      "7435/7435 [==============================] - 0s 62us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 18/25\n",
      "7435/7435 [==============================] - 0s 65us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 19/25\n",
      "7435/7435 [==============================] - 0s 63us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 20/25\n",
      "7435/7435 [==============================] - 0s 61us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 21/25\n",
      "7435/7435 [==============================] - 0s 62us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 22/25\n",
      "7435/7435 [==============================] - 0s 47us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 23/25\n",
      "7435/7435 [==============================] - 0s 47us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 24/25\n",
      "7435/7435 [==============================] - 0s 50us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "Epoch 25/25\n",
      "7435/7435 [==============================] - 0s 60us/step - loss: 57.7368 - acc: 0.0000e+00\n",
      "----------------------------------------------------------\n",
      "7435/7435 [==============================] - 0s 48us/step\n",
      "\n",
      "acc: 0.00%\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "--------------------------------------------\n",
      "13.357094821788836\n"
     ]
    }
   ],
   "source": [
    "#tutorial keras practice\n",
    "#https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Softmax\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy\n",
    "\n",
    "model = Sequential()\n",
    "#parameters = number of neurons, initialization method, activation function\n",
    "model.add(Dense(32, input_dim=76, init = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(16, init = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=25, batch_size=32)\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "scores = model.evaluate(x_train,y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "\n",
    "y_devpred = model.predict(x_dev)\n",
    "print(\"--------------------------------------------\")\n",
    "print(mean_squared_error(y_dev,y_devpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to implement/try:\n",
    "#### https://arxiv.org/pdf/1805.11317.pdf?fbclid=IwAR3QglQZ1iMLwGn1nenjOELFtXaNd8iql_sYTQO9uZbZJYRJ4ri7re0ggUY\n",
    "### Radial Basis Neural Network (RBFNN)\n",
    "### General Regression Neural Network (GRNN)\n",
    "# Back Propagation Neural Network (BPNN)\n",
    "\n",
    "## https://alphaarchitect.com/2018/06/05/machine-learning-financial-market-prediction-time-series-prediction-sklearn-keras/\n",
    "## https://nbviewer.jupyter.org/github/druce/Machine-learning-for-financial-market-prediction/blob/master/Replicate%20Paper%20by%20Rapach%2C%20Strauss%2C%20Tu%2C%20Zhou.ipynb\n",
    "# regression neural network, testing:\n",
    "# 1, 2, 3 layers.\n",
    "# 1, 2, 4, 8, 16 units per layer.\n",
    "# L1_L2 Regularization similar to ElasticNet, with penalties of 0.0, 0.1, 0.3, 1, 3, 10, 30, 100\n",
    "# author also suggests tryinga classification model sorting into buckets\n",
    "\n",
    "### General Ideas:\n",
    "# regression\n",
    "# multiclassification - softmax + cross entropy loss\n",
    "# binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_48_input to have 3 dimensions, but got array with shape (7435, 76)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-919446da4e26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0my_devpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--------------------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_48_input to have 3 dimensions, but got array with shape (7435, 76)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation = 'relu', input_shape = (x_train.shape)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "y_devpred = model.predict(x_dev)\n",
    "print(\"--------------------------------------------\")\n",
    "print(mean_squared_error(y_dev,y_devpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_18_input to have 3 dimensions, but got array with shape (7435, 76)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c35024846c8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m               loss='mse')\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0my_devpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--------------------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_18_input to have 3 dimensions, but got array with shape (7435, 76)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Softmax\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape = (x_train.shape)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "y_devpred = model.predict(x_dev)\n",
    "print(\"--------------------------------------------\")\n",
    "print(mean_squared_error(y_dev,y_devpred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_devpred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-eb48ad504332>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#8classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     precision[i], recall[i], _ = precision_recall_curve(y_dev,\n\u001b[1;32m---> 11\u001b[1;33m                                                         y_devpred)\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0maverage_precision\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_devpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_devpred' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(8): #8classes\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_dev,\n",
    "                                                        y_devpred)\n",
    "    average_precision[i] = average_precision_score(y_dev, y_devpred)\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_dev.ravel(),\n",
    "    y_devpred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_dev, y_devpred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "plt.plot(x_dev,np.abs(y_devpred-y_dev))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
