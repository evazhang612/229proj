{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#parse data \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#label encoding on categorical data \n",
    "\n",
    "#FAMA 49CRSP Common Stocks \n",
    "df = pd.read_csv('ee6d2f60cdafb550.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 25 25 ... 22 10 47]\n",
      "       public_date  FFI49_desc  NFIRM  indret_ew  indret_vw  dpr_Median  \\\n",
      "12337     19930831          25      6   0.052904   0.068678       0.460   \n",
      "12383     19930930          25      6  -0.035253  -0.046242       0.460   \n",
      "12430     19931031          25      6   0.009319  -0.018318       0.460   \n",
      "12477     19931130          25      5  -0.022123  -0.014000       0.479   \n",
      "12524     19931231          25      5   0.096039   0.072342       0.479   \n",
      "12571     19940131          25      5   0.043779   0.009888       0.479   \n",
      "12618     19940228          25      6   0.012589   0.039537       0.394   \n",
      "12665     19940331          25      6  -0.012399  -0.052069       0.394   \n",
      "12712     19940430          25      6  -0.026779  -0.024747       0.394   \n",
      "12759     19940531          25      6  -0.001201   0.023906       0.394   \n",
      "12806     19940630          25      6  -0.001294  -0.037881       0.394   \n",
      "12853     19940731          25      6   0.023423   0.043118       0.394   \n",
      "12900     19940831          25      6   0.066949   0.055253       0.398   \n",
      "12947     19940930          25      6   0.004471  -0.026728       0.398   \n",
      "12994     19941031          25      6  -0.044276  -0.015566       0.398   \n",
      "13041     19941130          25      6  -0.040811  -0.035578       0.390   \n",
      "13088     19941231          25      6   0.007111   0.043045       0.390   \n",
      "13135     19950131          25      6   0.021079   0.023549       0.390   \n",
      "13182     19950228          25      6   0.058941   0.058359       0.422   \n",
      "13229     19950331          25      6   0.057380   0.023958       0.422   \n",
      "13276     19950430          25      6   0.062681   0.029973       0.422   \n",
      "13323     19950531          25      6   0.037249   0.031139       0.378   \n",
      "13370     19950630          25      6   0.056587   0.052070       0.378   \n",
      "13417     19950731          25      6   0.004115  -0.005009       0.378   \n",
      "13464     19950831          25      6  -0.000317   0.011758       0.422   \n",
      "13511     19950930          25      6   0.053875   0.015358       0.422   \n",
      "13558     19951031          25      6   0.004551  -0.003330       0.422   \n",
      "13605     19951130          25      6   0.047691   0.086727       0.409   \n",
      "13652     19951231          25      6   0.026102   0.035340       0.409   \n",
      "13699     19960131          25      6   0.044772   0.033413       0.409   \n",
      "...            ...         ...    ...        ...        ...         ...   \n",
      "25133     20161231          38      3   0.053255   0.054318       0.851   \n",
      "25134     20161231          39      3   0.023255   0.024929       0.437   \n",
      "25135     20161231          40     28  -0.009651   0.000257       0.000   \n",
      "25136     20161231          41      1  -0.036863  -0.036863       0.840   \n",
      "25137     20161231          42     15   0.020397   0.053427       0.291   \n",
      "25138     20161231          43      3  -0.066658  -0.075610       0.454   \n",
      "25139     20161231          44     15   0.007842   0.007611       0.294   \n",
      "25124     20161231          27      6  -0.004840  -0.002528       0.551   \n",
      "25132     20161231          35     34  -0.025950  -0.006974       0.297   \n",
      "25123     20161231          26     12  -0.002451  -0.008172       0.401   \n",
      "25110     20161231          12      6  -0.015227  -0.017383       0.087   \n",
      "25120     20161231          23     10   0.015469   0.015504       0.443   \n",
      "25141     20161231          46     33   0.036605   0.036090       0.641   \n",
      "25098     20161231           0      6   0.013718   0.016368       0.273   \n",
      "25100     20161231           2      5   0.037148   0.024898       0.232   \n",
      "25101     20161231           3     28   0.044543   0.044528       0.377   \n",
      "25102     20161231           4      4   0.010696   0.033009       0.425   \n",
      "25103     20161231           5      5  -0.004710  -0.006343       0.274   \n",
      "25106     20161231           8     17   0.001487   0.000991       0.316   \n",
      "25122     20161231          25     12   0.005462   0.005478       0.162   \n",
      "25107     20161231           9     12   0.008697   0.006098       0.484   \n",
      "25109     20161231          11      7  -0.050234  -0.012120       0.274   \n",
      "25111     20161231          14     16   0.003128   0.009955       0.375   \n",
      "25113     20161231          16     13   0.023076   0.036328       0.370   \n",
      "25114     20161231          17     14   0.046219   0.053876       0.616   \n",
      "25117     20161231          20      1  -0.057719  -0.057719       0.560   \n",
      "25118     20161231          21      6   0.018384   0.012505       0.510   \n",
      "25119     20161231          22      6  -0.012955   0.001926       0.026   \n",
      "25108     20161231          10     20   0.014731   0.029014       0.417   \n",
      "25142     20161231          47      9   0.006330   0.004922       0.392   \n",
      "\n",
      "       PEG_trailing_Median  bm_Median  CAPEI_Median  divyield_Median  \\\n",
      "12337                4.556      0.438        21.835           0.0229   \n",
      "12383                4.380      0.438        20.573           0.0245   \n",
      "12430                4.268      0.438        20.959           0.0251   \n",
      "12477                3.865      0.401        19.861           0.0273   \n",
      "12524                4.122      0.401        21.179           0.0255   \n",
      "12571                4.190      0.401        21.483           0.0212   \n",
      "12618               11.991      0.406        22.148           0.0234   \n",
      "12665               11.971      0.406        20.340           0.0233   \n",
      "12712               11.623      0.406        19.663           0.0251   \n",
      "12759                3.700      0.393        19.092           0.0245   \n",
      "12806                3.833      0.393        18.900           0.0252   \n",
      "12853                3.825      0.393        19.442           0.0248   \n",
      "12900                1.836      0.397        19.539           0.0239   \n",
      "12947                1.765      0.397        18.768           0.0239   \n",
      "12994                1.664      0.397        18.101           0.0244   \n",
      "13041                1.371      0.382        17.360           0.0269   \n",
      "13088                1.447      0.382        18.339           0.0271   \n",
      "13135                1.376      0.382        18.007           0.0260   \n",
      "13182                1.572      0.390        19.144           0.0249   \n",
      "13229                1.628      0.390        19.835           0.0246   \n",
      "13276                1.653      0.395        25.704           0.0237   \n",
      "13323                1.430      0.359        26.911           0.0223   \n",
      "13370                1.520      0.359        27.939           0.0212   \n",
      "13417                1.564      0.359        28.178           0.0217   \n",
      "13464                1.358      0.334        27.136           0.0214   \n",
      "13511                1.124      0.334        27.864           0.0212   \n",
      "13558                1.254      0.334        27.607           0.0216   \n",
      "13605                1.052      0.348        28.600           0.0204   \n",
      "13652                1.076      0.348        30.916           0.0197   \n",
      "13699                1.046      0.339        31.812           0.0174   \n",
      "...                    ...        ...           ...              ...   \n",
      "25133                5.987      0.273        28.409           0.0361   \n",
      "25134                0.997      0.166        24.194           0.0286   \n",
      "25135                1.724      0.177        27.234           0.0141   \n",
      "25136                3.066      0.523        33.958           0.0254   \n",
      "25137                0.765      0.502        22.358           0.0197   \n",
      "25138                0.611      0.233        25.309           0.0262   \n",
      "25139                1.132      0.432        21.286           0.0153   \n",
      "25124                1.497      0.101        31.726           0.0308   \n",
      "25132                1.589      0.305        21.762           0.0206   \n",
      "25123                0.807      0.312        16.528           0.0202   \n",
      "25110                0.500      0.655        14.286           0.0153   \n",
      "25120                3.701      0.212        26.668           0.0239   \n",
      "25141                2.104      0.798        23.026           0.0335   \n",
      "25098                1.627      0.241        22.032           0.0176   \n",
      "25100                0.446      0.542         9.784           0.0244   \n",
      "25101                2.449      0.877        15.996           0.0182   \n",
      "25102                8.059      0.185        26.059           0.0166   \n",
      "25103                0.568      0.293        41.512           0.0166   \n",
      "25106                1.826      0.219        32.407           0.0158   \n",
      "25122                2.457      0.265        23.877           0.0066   \n",
      "25107                1.148      0.295        24.150           0.0228   \n",
      "25109                0.845      0.210        19.595           0.0182   \n",
      "25111                1.298      0.178        24.618           0.0274   \n",
      "25113                1.429      0.596        22.645           0.0202   \n",
      "25114                1.332      0.219        25.145           0.0234   \n",
      "25117                2.528      0.033        23.666           0.0291   \n",
      "25118                2.234      0.411        18.186           0.0319   \n",
      "25119                1.222      0.396        21.419           0.0106   \n",
      "25108                1.664      0.262        24.592           0.0206   \n",
      "25142                3.060      0.244        22.458           0.0224   \n",
      "\n",
      "               ...            rect_turn_Median  sale_equity_Median  \\\n",
      "12337          ...                       5.494               3.146   \n",
      "12383          ...                       5.494               3.146   \n",
      "12430          ...                       5.685               3.027   \n",
      "12477          ...                       5.364               2.755   \n",
      "12524          ...                       5.364               2.755   \n",
      "12571          ...                       5.319               2.795   \n",
      "12618          ...                       4.789               2.451   \n",
      "12665          ...                       4.789               2.451   \n",
      "12712          ...                       4.789               2.525   \n",
      "12759          ...                       4.704               2.516   \n",
      "12806          ...                       4.704               2.516   \n",
      "12853          ...                       4.704               2.448   \n",
      "12900          ...                       4.589               2.449   \n",
      "12947          ...                       4.589               2.449   \n",
      "12994          ...                       4.589               2.513   \n",
      "13041          ...                       4.454               2.458   \n",
      "13088          ...                       4.454               2.458   \n",
      "13135          ...                       4.454               2.435   \n",
      "13182          ...                       4.443               2.568   \n",
      "13229          ...                       4.443               2.568   \n",
      "13276          ...                       4.443               2.526   \n",
      "13323          ...                       4.521               2.463   \n",
      "13370          ...                       4.521               2.463   \n",
      "13417          ...                       4.521               2.282   \n",
      "13464          ...                       4.551               2.364   \n",
      "13511          ...                       4.551               2.364   \n",
      "13558          ...                       4.551               2.510   \n",
      "13605          ...                       4.621               2.689   \n",
      "13652          ...                       4.621               2.689   \n",
      "13699          ...                       4.621               2.692   \n",
      "...            ...                         ...                 ...   \n",
      "25133          ...                      84.444               3.603   \n",
      "25134          ...                       9.870               1.646   \n",
      "25135          ...                       6.467               1.324   \n",
      "25136          ...                       9.640               2.084   \n",
      "25137          ...                       6.513               1.331   \n",
      "25138          ...                       5.333               2.850   \n",
      "25139          ...                      11.577               3.411   \n",
      "25124          ...                      24.406               3.634   \n",
      "25132          ...                      57.162               4.892   \n",
      "25123          ...                       4.811               1.908   \n",
      "25110          ...                      15.815               2.066   \n",
      "25120          ...                       7.705               3.133   \n",
      "25141          ...                       9.204               0.835   \n",
      "25098          ...                       6.365               2.672   \n",
      "25100          ...                       5.036               3.614   \n",
      "25101          ...                       0.092               0.422   \n",
      "25102          ...                       6.947               1.606   \n",
      "25103          ...                       7.212               1.779   \n",
      "25106          ...                       4.239               2.062   \n",
      "25122          ...                       6.038               1.086   \n",
      "25107          ...                       5.477               1.787   \n",
      "25109          ...                       9.522               2.378   \n",
      "25111          ...                       5.736               1.169   \n",
      "25113          ...                       4.244               0.499   \n",
      "25114          ...                      11.066               2.545   \n",
      "25117          ...                       5.158              15.531   \n",
      "25118          ...                       5.850               1.397   \n",
      "25119          ...                       7.180               1.817   \n",
      "25108          ...                       7.178               0.982   \n",
      "25142          ...                       9.550               4.712   \n",
      "\n",
      "       sale_invcap_Median  sale_nwc_Median  accrual_Median  rd_sale_Median  \\\n",
      "12337               2.601            9.285           0.049           0.068   \n",
      "12383               2.601            9.285           0.056           0.042   \n",
      "12430               2.484            9.285           0.056           0.042   \n",
      "12477               2.345            8.303           0.057           0.080   \n",
      "12524               2.345            8.303           0.057           0.080   \n",
      "12571               2.347            8.303           0.057           0.080   \n",
      "12618               2.121            7.341           0.038           0.068   \n",
      "12665               2.121            7.341           0.038           0.068   \n",
      "12712               2.163            7.321           0.038           0.068   \n",
      "12759               2.165            7.181           0.029           0.066   \n",
      "12806               2.165            7.181           0.029           0.066   \n",
      "12853               2.098            6.644           0.018           0.066   \n",
      "12900               2.110            6.244           0.001           0.064   \n",
      "12947               2.110            6.244           0.001           0.064   \n",
      "12994               2.162            6.483          -0.012           0.064   \n",
      "13041               2.132            6.226           0.012           0.063   \n",
      "13088               2.132            6.226           0.012           0.063   \n",
      "13135               2.121            6.171           0.012           0.063   \n",
      "13182               2.167            5.887           0.018           0.061   \n",
      "13229               2.167            5.887           0.018           0.061   \n",
      "13276               2.141            5.817           0.018           0.061   \n",
      "13323               2.157            5.438           0.016           0.059   \n",
      "13370               2.157            5.438           0.016           0.059   \n",
      "13417               2.032            5.562           0.016           0.059   \n",
      "13464               2.043            5.071           0.012           0.058   \n",
      "13511               2.043            5.071           0.012           0.058   \n",
      "13558               2.141            5.032           0.012           0.058   \n",
      "13605               2.123            5.813           0.007           0.056   \n",
      "13652               2.123            5.813           0.007           0.056   \n",
      "13699               2.116            5.819           0.007           0.056   \n",
      "...                   ...              ...             ...             ...   \n",
      "25133               1.202           37.073           0.000           0.010   \n",
      "25134               0.771            5.962           0.010           0.000   \n",
      "25135               0.848            2.536           0.092           0.142   \n",
      "25136               1.283            3.447           0.067           0.000   \n",
      "25137               0.642            5.943           0.029           0.000   \n",
      "25138               1.539            4.376           0.042           0.053   \n",
      "25139               1.975           22.973           0.054           0.000   \n",
      "25124               2.345           16.491           0.096           0.000   \n",
      "25132               2.979           11.434           0.064           0.000   \n",
      "25123               1.083            3.513           0.044           0.027   \n",
      "25110               1.686            9.334           0.022           0.000   \n",
      "25120               1.532           22.271           0.045           0.018   \n",
      "25141               0.412           28.583           0.048           0.000   \n",
      "25098               1.552            6.191           0.004           0.042   \n",
      "25100               1.527           10.102           0.052           0.038   \n",
      "25101               0.242            2.369           0.005           0.000   \n",
      "25102               0.768            4.789           0.023           0.000   \n",
      "25103               1.288            5.502           0.033           0.010   \n",
      "25106               0.837            8.357           0.043           0.000   \n",
      "25122               0.717            5.891           0.027           0.056   \n",
      "25107               0.819            5.277           0.045           0.028   \n",
      "25109               1.681            4.177           0.023           0.000   \n",
      "25111               0.771            2.366           0.035           0.174   \n",
      "25113               0.336            7.101           0.007           0.000   \n",
      "25114               1.555           12.893           0.039           0.007   \n",
      "25117               2.612           29.236           0.046           0.018   \n",
      "25118               0.955            2.551           0.032           0.130   \n",
      "25119               0.932           13.276           0.053           0.000   \n",
      "25108               0.838            2.092           0.052           0.099   \n",
      "25142               4.343            9.503           0.040           0.000   \n",
      "\n",
      "       adv_sale_Median  staff_sale_Median  PEG_1yrforward_Median  \\\n",
      "12337            0.013              0.000                 -0.333   \n",
      "12383            0.013              0.000                 -0.320   \n",
      "12430            0.013              0.000                 -0.350   \n",
      "12477            0.016              0.000                  0.677   \n",
      "12524            0.016              0.000                  0.722   \n",
      "12571            0.016              0.000                  0.814   \n",
      "12618            0.011              0.000                  1.108   \n",
      "12665            0.011              0.000                  1.013   \n",
      "12712            0.011              0.000                  0.897   \n",
      "12759            0.011              0.000                  0.816   \n",
      "12806            0.011              0.000                  0.866   \n",
      "12853            0.011              0.000                  1.522   \n",
      "12900            0.012              0.000                  1.517   \n",
      "12947            0.012              0.000                  1.456   \n",
      "12994            0.012              0.000                  0.806   \n",
      "13041            0.008              0.000                  0.923   \n",
      "13088            0.008              0.000                  0.975   \n",
      "13135            0.008              0.000                  1.063   \n",
      "13182            0.000              0.000                  1.046   \n",
      "13229            0.000              0.000                  1.123   \n",
      "13276            0.000              0.000                  1.136   \n",
      "13323            0.000              0.000                  1.055   \n",
      "13370            0.000              0.000                  1.134   \n",
      "13417            0.000              0.000                  1.098   \n",
      "13464            0.000              0.000                  1.022   \n",
      "13511            0.000              0.000                  1.002   \n",
      "13558            0.000              0.000                  0.935   \n",
      "13605            0.000              0.000                  0.873   \n",
      "13652            0.000              0.000                  0.863   \n",
      "13699            0.000              0.000                  0.840   \n",
      "...                ...                ...                    ...   \n",
      "25133            0.013              0.000                  3.095   \n",
      "25134            0.078              0.000                  0.567   \n",
      "25135            0.017              0.000                  1.762   \n",
      "25136            0.000              0.000                  1.132   \n",
      "25137            0.040              0.000                  0.458   \n",
      "25138            0.092              0.000                  8.788   \n",
      "25139            0.000              0.245                  2.168   \n",
      "25124            0.022              0.087                  1.605   \n",
      "25132            0.011              0.000                  1.195   \n",
      "25123            0.000              0.000                 -0.713   \n",
      "25110            0.002              0.000                  0.931   \n",
      "25120            0.056              0.000                  3.005   \n",
      "25141            0.000              0.000                  2.240   \n",
      "25098            0.000              0.000                  2.150   \n",
      "25100            0.023              0.000                  0.164   \n",
      "25101            0.015              0.306                  1.301   \n",
      "25102            0.092              0.000                  2.981   \n",
      "25103            0.000              0.000                  0.718   \n",
      "25106            0.000              0.000                  2.369   \n",
      "25122            0.000              0.000                  2.091   \n",
      "25107            0.000              0.000                  1.718   \n",
      "25109            0.047              0.000                  2.371   \n",
      "25111            0.014              0.000                  1.328   \n",
      "25113            0.000              0.242                  1.433   \n",
      "25114            0.029              0.000                  2.894   \n",
      "25117            0.000              0.000                  2.765   \n",
      "25118            0.004              0.000                  1.552   \n",
      "25119            0.000              0.228                  1.508   \n",
      "25108            0.000              0.000                  1.713   \n",
      "25142            0.000              0.000                 -4.237   \n",
      "\n",
      "       PEG_ltgforward_Median  \n",
      "12337                  1.572  \n",
      "12383                  1.445  \n",
      "12430                  1.422  \n",
      "12477                  1.848  \n",
      "12524                  1.962  \n",
      "12571                  1.930  \n",
      "12618                  1.721  \n",
      "12665                  1.587  \n",
      "12712                  1.557  \n",
      "12759                  1.410  \n",
      "12806                  1.438  \n",
      "12853                  1.525  \n",
      "12900                  1.538  \n",
      "12947                  1.524  \n",
      "12994                  1.418  \n",
      "13041                  1.333  \n",
      "13088                  1.429  \n",
      "13135                  1.478  \n",
      "13182                  1.517  \n",
      "13229                  1.610  \n",
      "13276                  1.671  \n",
      "13323                  1.668  \n",
      "13370                  1.722  \n",
      "13417                  1.679  \n",
      "13464                  1.563  \n",
      "13511                  1.687  \n",
      "13558                  1.653  \n",
      "13605                  1.633  \n",
      "13652                  1.570  \n",
      "13699                  1.565  \n",
      "...                      ...  \n",
      "25133                  3.081  \n",
      "25134                  1.997  \n",
      "25135                  2.425  \n",
      "25136                  1.401  \n",
      "25137                  1.019  \n",
      "25138                  2.122  \n",
      "25139                  2.081  \n",
      "25124                  1.987  \n",
      "25132                  1.778  \n",
      "25123                  2.365  \n",
      "25110                  1.580  \n",
      "25120                  3.063  \n",
      "25141                  3.676  \n",
      "25098                  2.645  \n",
      "25100                 -1.294  \n",
      "25101                  1.945  \n",
      "25102                  3.205  \n",
      "25103                  1.579  \n",
      "25106                  2.179  \n",
      "25122                  2.545  \n",
      "25107                  2.572  \n",
      "25109                  2.236  \n",
      "25111                  2.379  \n",
      "25113                  1.546  \n",
      "25114                  3.445  \n",
      "25117                  2.721  \n",
      "25118                  1.875  \n",
      "25119                  1.884  \n",
      "25108                  2.218  \n",
      "25142                  2.072  \n",
      "\n",
      "[9297 rows x 76 columns]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing \n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#preprocessing here\n",
    "#sort by date \n",
    "df = df.sort_values(by = 'public_date', ascending = True)\n",
    "\n",
    "#encode integer categories into numbers \n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df.FFI49_desc)\n",
    "print(integer_encoded)\n",
    "df.FFI49_desc = integer_encoded\n",
    "df.divyield_Median = [float(x.strip('%'))/100 for x in df.divyield_Median]\n",
    "print(df)\n",
    "\n",
    "#todo: https://www.gsb.stanford.edu/library/articles/databases/links/financial-ratios-suite?fbclid=IwAR0EGNGk9DdxQjEHfdaoUhdY3tNzAWDogYDzuuJi1zT_muL-uJtWQw19Fzk\n",
    "\n",
    "#get output first \n",
    "ewlabels = df.indret_ew\n",
    "vwlabels = df.indret_vw\n",
    "\n",
    "#3year on year change as a prediction feature, raw pct change \n",
    "yoythree = ewlabels.diff(periods = 3)\n",
    "#3 years rolling percent change, averaged ie. (y1-y2 + (y3-y2)change)/2 \n",
    "rollavgpct = ewlabels.rolling(3).mean()\n",
    "\n",
    "#drop first 3 years\n",
    "df = df.iloc[3:]\n",
    "ewlabels = ewlabels.iloc[3:]\n",
    "yoythree = yoythree.iloc[3:]\n",
    "#yoypctthree = yoypctthree.iloc[3:]\n",
    "rollavgpct = rollavgpct.iloc[3:]\n",
    "\n",
    "#add -1 and 1 so the bins will take on bins to be equal and set to max -1 and 1\n",
    "extrema = pd.Series([-1,1])\n",
    "ewnlabels = ewlabels.append(extrema)\n",
    "\n",
    "#make a new output (bucket by percentage?)\n",
    "enc = KBinsDiscretizer(n_bins=8, encode='ordinal',strategy = 'uniform')\n",
    "ewnlabels = np.asarray(ewnlabels)\n",
    "ewnlabels = ewnlabels.reshape((-1,1))\n",
    "labels_binned = enc.fit_transform(ewnlabels)\n",
    "\n",
    "labels_binned = labels_binned[:-2]\n",
    "\n",
    "#1 Split-Timer series data, 0.64 Train, 0.16 dev, 0.2 Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, labels_binned, test_size = 0.2, shuffle = False)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(df, ewnlabels, test_size = 0.2, shuffle = False)\n",
    "x_tra, x_dev, y_tra, y_dev = train_test_split(x_train, y_train, test_size = 0.2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       public_date  FFI49_desc  NFIRM  indret_ew  indret_vw  dpr_Median  \\\n",
      "12477     19931130          25      5  -0.022123  -0.014000       0.479   \n",
      "12524     19931231          25      5   0.096039   0.072342       0.479   \n",
      "12571     19940131          25      5   0.043779   0.009888       0.479   \n",
      "12618     19940228          25      6   0.012589   0.039537       0.394   \n",
      "12665     19940331          25      6  -0.012399  -0.052069       0.394   \n",
      "12712     19940430          25      6  -0.026779  -0.024747       0.394   \n",
      "12759     19940531          25      6  -0.001201   0.023906       0.394   \n",
      "12806     19940630          25      6  -0.001294  -0.037881       0.394   \n",
      "12853     19940731          25      6   0.023423   0.043118       0.394   \n",
      "12900     19940831          25      6   0.066949   0.055253       0.398   \n",
      "12947     19940930          25      6   0.004471  -0.026728       0.398   \n",
      "12994     19941031          25      6  -0.044276  -0.015566       0.398   \n",
      "13041     19941130          25      6  -0.040811  -0.035578       0.390   \n",
      "13088     19941231          25      6   0.007111   0.043045       0.390   \n",
      "13135     19950131          25      6   0.021079   0.023549       0.390   \n",
      "13182     19950228          25      6   0.058941   0.058359       0.422   \n",
      "13229     19950331          25      6   0.057380   0.023958       0.422   \n",
      "13276     19950430          25      6   0.062681   0.029973       0.422   \n",
      "13323     19950531          25      6   0.037249   0.031139       0.378   \n",
      "13370     19950630          25      6   0.056587   0.052070       0.378   \n",
      "13417     19950731          25      6   0.004115  -0.005009       0.378   \n",
      "13464     19950831          25      6  -0.000317   0.011758       0.422   \n",
      "13511     19950930          25      6   0.053875   0.015358       0.422   \n",
      "13558     19951031          25      6   0.004551  -0.003330       0.422   \n",
      "13605     19951130          25      6   0.047691   0.086727       0.409   \n",
      "13652     19951231          25      6   0.026102   0.035340       0.409   \n",
      "13699     19960131          25      6   0.044772   0.033413       0.409   \n",
      "13745     19960229          25      6   0.003002  -0.025817       0.382   \n",
      "13791     19960331          25      6  -0.020940   0.020593       0.382   \n",
      "13837     19960430          25      6   0.057884   0.024875       0.382   \n",
      "...            ...         ...    ...        ...        ...         ...   \n",
      "21410     20100331           4      5   0.078739   0.059782       0.423   \n",
      "21411     20100331           5      6   0.082778   0.090577       0.543   \n",
      "21412     20100331           6      4   0.068238   0.055803       0.105   \n",
      "21413     20100331           7      2   0.093421   0.092510       0.049   \n",
      "21414     20100331           8     13   0.076576   0.069945       0.239   \n",
      "21406     20100331           0      7   0.103798   0.108410       0.312   \n",
      "21416     20100331          10     30   0.074012   0.078976       0.243   \n",
      "21417     20100331          11      5   0.084022   0.083646       0.329   \n",
      "21418     20100331          12      6   0.061611   0.069313       0.000   \n",
      "21419     20100331          13      3   0.018897  -0.026772       0.152   \n",
      "21420     20100331          14     21   0.035427   0.019780       0.000   \n",
      "21421     20100331          15      3   0.072547   0.070492       0.872   \n",
      "21422     20100331          16     14   0.083292   0.075081       0.362   \n",
      "21423     20100331          17     15   0.042380   0.036261       0.437   \n",
      "21415     20100331           9     11   0.060225   0.072472       0.469   \n",
      "21483     20100430          31      4   0.051587   0.040814       0.286   \n",
      "21482     20100430          30     24   0.034347   0.037883       0.274   \n",
      "21481     20100430          29      3  -0.000077  -0.063170       0.156   \n",
      "21477     20100430          25      9   0.062083   0.060680       0.018   \n",
      "21479     20100430          27      6   0.098346   0.081673       0.336   \n",
      "21478     20100430          26     13   0.049836   0.058474       0.329   \n",
      "21484     20100430          32      8   0.044267   0.038288       0.497   \n",
      "21480     20100430          28     12   0.016479  -0.001779       0.108   \n",
      "21485     20100430          33      5   0.053862   0.012300       0.403   \n",
      "21492     20100430          41      6  -0.090854  -0.062553       1.114   \n",
      "21489     20100430          38      4   0.001207  -0.024248       0.772   \n",
      "21491     20100430          40     28   0.013151   0.001585       0.000   \n",
      "21493     20100430          42     19   0.047229   0.028011       0.333   \n",
      "21494     20100430          43      3  -0.045039  -0.021469       0.408   \n",
      "21495     20100430          44      9   0.039780   0.042859       0.335   \n",
      "\n",
      "       PEG_trailing_Median  bm_Median  CAPEI_Median  divyield_Median  \\\n",
      "12477                3.865      0.401        19.861           0.0273   \n",
      "12524                4.122      0.401        21.179           0.0255   \n",
      "12571                4.190      0.401        21.483           0.0212   \n",
      "12618               11.991      0.406        22.148           0.0234   \n",
      "12665               11.971      0.406        20.340           0.0233   \n",
      "12712               11.623      0.406        19.663           0.0251   \n",
      "12759                3.700      0.393        19.092           0.0245   \n",
      "12806                3.833      0.393        18.900           0.0252   \n",
      "12853                3.825      0.393        19.442           0.0248   \n",
      "12900                1.836      0.397        19.539           0.0239   \n",
      "12947                1.765      0.397        18.768           0.0239   \n",
      "12994                1.664      0.397        18.101           0.0244   \n",
      "13041                1.371      0.382        17.360           0.0269   \n",
      "13088                1.447      0.382        18.339           0.0271   \n",
      "13135                1.376      0.382        18.007           0.0260   \n",
      "13182                1.572      0.390        19.144           0.0249   \n",
      "13229                1.628      0.390        19.835           0.0246   \n",
      "13276                1.653      0.395        25.704           0.0237   \n",
      "13323                1.430      0.359        26.911           0.0223   \n",
      "13370                1.520      0.359        27.939           0.0212   \n",
      "13417                1.564      0.359        28.178           0.0217   \n",
      "13464                1.358      0.334        27.136           0.0214   \n",
      "13511                1.124      0.334        27.864           0.0212   \n",
      "13558                1.254      0.334        27.607           0.0216   \n",
      "13605                1.052      0.348        28.600           0.0204   \n",
      "13652                1.076      0.348        30.916           0.0197   \n",
      "13699                1.046      0.339        31.812           0.0174   \n",
      "13745                0.043      0.305        28.287           0.0172   \n",
      "13791                0.050      0.305        26.299           0.0186   \n",
      "13837                0.051      0.305        28.086           0.0175   \n",
      "...                    ...        ...           ...              ...   \n",
      "21410                2.764      0.264        16.811           0.0256   \n",
      "21411                0.756      0.590        15.854           0.0175   \n",
      "21412               47.898      0.397         2.674           0.0264   \n",
      "21413                1.252      0.325       243.770           0.0075   \n",
      "21414                0.878      0.475        20.651           0.0179   \n",
      "21406                2.382      0.313        17.894           0.0153   \n",
      "21416                0.525      0.350        17.863           0.0191   \n",
      "21417                2.972      0.385        16.232           0.0147   \n",
      "21418                0.220      0.748         4.729           0.0108   \n",
      "21419                0.798      0.334        21.381           0.0061   \n",
      "21420                0.592      0.344        21.225           0.0304   \n",
      "21421               27.928      0.290        16.364           0.0236   \n",
      "21422                4.623      0.816        18.291           0.0128   \n",
      "21423                1.140      0.379        19.072           0.0279   \n",
      "21415                1.992      0.314        17.306           0.0213   \n",
      "21483                0.386      0.561        27.768           0.0304   \n",
      "21482                0.536      0.753        12.894           0.0101   \n",
      "21481                6.223      0.433        27.429           0.0079   \n",
      "21477                1.569      0.377        24.980           0.0087   \n",
      "21479                0.930      0.211        22.424           0.0176   \n",
      "21478                0.924      0.395        17.430           0.0169   \n",
      "21484                1.116      0.558        20.549           0.0237   \n",
      "21480                1.013      0.325        21.219           0.0104   \n",
      "21485                0.433      0.272        18.083           0.0196   \n",
      "21492               -0.064      0.509        12.455           0.0119   \n",
      "21489                1.522      0.196        14.002           0.0586   \n",
      "21491                0.978      0.279        30.421           0.0124   \n",
      "21493                0.975      0.704        15.556           0.0284   \n",
      "21494                3.733      0.364        18.511           0.0293   \n",
      "21495                2.373      0.594        21.950           0.0143   \n",
      "\n",
      "               ...            rect_turn_Median  sale_equity_Median  \\\n",
      "12477          ...                       5.364               2.755   \n",
      "12524          ...                       5.364               2.755   \n",
      "12571          ...                       5.319               2.795   \n",
      "12618          ...                       4.789               2.451   \n",
      "12665          ...                       4.789               2.451   \n",
      "12712          ...                       4.789               2.525   \n",
      "12759          ...                       4.704               2.516   \n",
      "12806          ...                       4.704               2.516   \n",
      "12853          ...                       4.704               2.448   \n",
      "12900          ...                       4.589               2.449   \n",
      "12947          ...                       4.589               2.449   \n",
      "12994          ...                       4.589               2.513   \n",
      "13041          ...                       4.454               2.458   \n",
      "13088          ...                       4.454               2.458   \n",
      "13135          ...                       4.454               2.435   \n",
      "13182          ...                       4.443               2.568   \n",
      "13229          ...                       4.443               2.568   \n",
      "13276          ...                       4.443               2.526   \n",
      "13323          ...                       4.521               2.463   \n",
      "13370          ...                       4.521               2.463   \n",
      "13417          ...                       4.521               2.282   \n",
      "13464          ...                       4.551               2.364   \n",
      "13511          ...                       4.551               2.364   \n",
      "13558          ...                       4.551               2.510   \n",
      "13605          ...                       4.621               2.689   \n",
      "13652          ...                       4.621               2.689   \n",
      "13699          ...                       4.621               2.692   \n",
      "13745          ...                       4.713               2.683   \n",
      "13791          ...                       4.713               2.683   \n",
      "13837          ...                       4.713               2.673   \n",
      "...            ...                         ...                 ...   \n",
      "21410          ...                       5.642               1.325   \n",
      "21411          ...                       6.260               1.879   \n",
      "21412          ...                       6.528               3.361   \n",
      "21413          ...                       9.197               4.620   \n",
      "21414          ...                       5.295               1.489   \n",
      "21406          ...                       6.010               2.946   \n",
      "21416          ...                       7.904               1.154   \n",
      "21417          ...                       9.419               1.893   \n",
      "21418          ...                       6.727               1.417   \n",
      "21419          ...                      14.468               2.142   \n",
      "21420          ...                       6.007               0.924   \n",
      "21421          ...                       5.644               2.281   \n",
      "21422          ...                       6.817               0.686   \n",
      "21423          ...                      11.937               4.262   \n",
      "21415          ...                       5.592               2.626   \n",
      "21483          ...                       7.139               1.634   \n",
      "21482          ...                       6.127               0.644   \n",
      "21481          ...                       9.977               0.923   \n",
      "21477          ...                       5.869               1.101   \n",
      "21479          ...                      28.244               3.780   \n",
      "21478          ...                       4.698               1.575   \n",
      "21484          ...                       6.947               2.683   \n",
      "21480          ...                       5.751               1.152   \n",
      "21485          ...                      10.043               3.425   \n",
      "21492          ...                       7.175               1.516   \n",
      "21489          ...                      49.648               4.257   \n",
      "21491          ...                       5.808               1.110   \n",
      "21493          ...                       8.898               1.378   \n",
      "21494          ...                       6.412               2.551   \n",
      "21495          ...                       9.141               1.894   \n",
      "\n",
      "       sale_invcap_Median  sale_nwc_Median  accrual_Median  rd_sale_Median  \\\n",
      "12477               2.345            8.303           0.057           0.080   \n",
      "12524               2.345            8.303           0.057           0.080   \n",
      "12571               2.347            8.303           0.057           0.080   \n",
      "12618               2.121            7.341           0.038           0.068   \n",
      "12665               2.121            7.341           0.038           0.068   \n",
      "12712               2.163            7.321           0.038           0.068   \n",
      "12759               2.165            7.181           0.029           0.066   \n",
      "12806               2.165            7.181           0.029           0.066   \n",
      "12853               2.098            6.644           0.018           0.066   \n",
      "12900               2.110            6.244           0.001           0.064   \n",
      "12947               2.110            6.244           0.001           0.064   \n",
      "12994               2.162            6.483          -0.012           0.064   \n",
      "13041               2.132            6.226           0.012           0.063   \n",
      "13088               2.132            6.226           0.012           0.063   \n",
      "13135               2.121            6.171           0.012           0.063   \n",
      "13182               2.167            5.887           0.018           0.061   \n",
      "13229               2.167            5.887           0.018           0.061   \n",
      "13276               2.141            5.817           0.018           0.061   \n",
      "13323               2.157            5.438           0.016           0.059   \n",
      "13370               2.157            5.438           0.016           0.059   \n",
      "13417               2.032            5.562           0.016           0.059   \n",
      "13464               2.043            5.071           0.012           0.058   \n",
      "13511               2.043            5.071           0.012           0.058   \n",
      "13558               2.141            5.032           0.012           0.058   \n",
      "13605               2.123            5.813           0.007           0.056   \n",
      "13652               2.123            5.813           0.007           0.056   \n",
      "13699               2.116            5.819           0.007           0.056   \n",
      "13745               2.135            6.049           0.022           0.055   \n",
      "13791               2.135            6.049           0.022           0.055   \n",
      "13837               2.118            6.007           0.022           0.055   \n",
      "...                   ...              ...             ...             ...   \n",
      "21410               1.019            8.091           0.031           0.000   \n",
      "21411               1.145            4.666           0.067           0.010   \n",
      "21412               1.592           37.687           0.086           0.000   \n",
      "21413               1.657           12.058           0.052           0.006   \n",
      "21414               1.489            7.596           0.057           0.000   \n",
      "21406               1.782            7.494           0.030           0.038   \n",
      "21416               0.917            2.232           0.081           0.139   \n",
      "21417               1.519            3.373           0.093           0.000   \n",
      "21418               0.868            7.559           0.085           0.000   \n",
      "21419               1.055            4.977           0.054           0.000   \n",
      "21420               0.711            2.479           0.048           0.157   \n",
      "21421               1.515            6.470           0.087           0.033   \n",
      "21422               0.472            2.022           0.022           0.000   \n",
      "21423               1.876           11.184           0.056           0.005   \n",
      "21415               1.491            5.110           0.070           0.027   \n",
      "21483               0.624           45.613           0.049           0.000   \n",
      "21482               0.508           11.527           0.087           0.000   \n",
      "21481               0.738            3.683           0.050           0.000   \n",
      "21477               0.770            3.428           0.054           0.059   \n",
      "21479               2.355           28.506           0.088           0.000   \n",
      "21478               1.040            3.141           0.069           0.031   \n",
      "21484               1.404            6.602           0.080           0.013   \n",
      "21480               0.947            2.815           0.051           0.061   \n",
      "21485               1.741           13.704           0.105           0.000   \n",
      "21492               1.013            3.589           0.073           0.004   \n",
      "21489               1.182            7.145           0.032           0.009   \n",
      "21491               0.921            2.581           0.077           0.121   \n",
      "21493               0.608            6.353           0.082           0.000   \n",
      "21494               1.681            3.636           0.088           0.045   \n",
      "21495               1.177           13.908           0.041           0.000   \n",
      "\n",
      "       adv_sale_Median  staff_sale_Median  PEG_1yrforward_Median  \\\n",
      "12477            0.016              0.000                  0.677   \n",
      "12524            0.016              0.000                  0.722   \n",
      "12571            0.016              0.000                  0.814   \n",
      "12618            0.011              0.000                  1.108   \n",
      "12665            0.011              0.000                  1.013   \n",
      "12712            0.011              0.000                  0.897   \n",
      "12759            0.011              0.000                  0.816   \n",
      "12806            0.011              0.000                  0.866   \n",
      "12853            0.011              0.000                  1.522   \n",
      "12900            0.012              0.000                  1.517   \n",
      "12947            0.012              0.000                  1.456   \n",
      "12994            0.012              0.000                  0.806   \n",
      "13041            0.008              0.000                  0.923   \n",
      "13088            0.008              0.000                  0.975   \n",
      "13135            0.008              0.000                  1.063   \n",
      "13182            0.000              0.000                  1.046   \n",
      "13229            0.000              0.000                  1.123   \n",
      "13276            0.000              0.000                  1.136   \n",
      "13323            0.000              0.000                  1.055   \n",
      "13370            0.000              0.000                  1.134   \n",
      "13417            0.000              0.000                  1.098   \n",
      "13464            0.000              0.000                  1.022   \n",
      "13511            0.000              0.000                  1.002   \n",
      "13558            0.000              0.000                  0.935   \n",
      "13605            0.000              0.000                  0.873   \n",
      "13652            0.000              0.000                  0.863   \n",
      "13699            0.000              0.000                  0.840   \n",
      "13745            0.000              0.000                  0.906   \n",
      "13791            0.000              0.000                  1.060   \n",
      "13837            0.000              0.000                  1.169   \n",
      "...                ...                ...                    ...   \n",
      "21410            0.090              0.000                  1.135   \n",
      "21411            0.007              0.000                  0.602   \n",
      "21412            0.005              0.000                  1.236   \n",
      "21413            0.000              0.000                  2.336   \n",
      "21414            0.000              0.000                  1.709   \n",
      "21406            0.000              0.000                 -1.855   \n",
      "21416            0.000              0.000                  0.260   \n",
      "21417            0.034              0.000                 -1.109   \n",
      "21418            0.004              0.000                 -0.088   \n",
      "21419            0.000              0.000                  0.560   \n",
      "21420            0.000              0.000                  1.528   \n",
      "21421            0.000              0.000                  2.434   \n",
      "21422            0.000              0.232                  0.380   \n",
      "21423            0.022              0.000                  1.214   \n",
      "21415            0.000              0.000                  1.271   \n",
      "21483            0.000              0.000                  1.908   \n",
      "21482            0.000              0.000                 -0.123   \n",
      "21481            0.000              0.000                  0.360   \n",
      "21477            0.001              0.000                  1.735   \n",
      "21479            0.027              0.000                  1.519   \n",
      "21478            0.000              0.000                  0.214   \n",
      "21484            0.000              0.000                  1.433   \n",
      "21480            0.000              0.000                  2.045   \n",
      "21485            0.000              0.000                  0.470   \n",
      "21492            0.000              0.000                 -0.078   \n",
      "21489            0.012              0.000                  1.891   \n",
      "21491            0.012              0.000                  1.848   \n",
      "21493            0.024              0.000                  1.122   \n",
      "21494            0.101              0.000                  0.777   \n",
      "21495            0.000              0.291                  0.927   \n",
      "\n",
      "       PEG_ltgforward_Median  \n",
      "12477                  1.848  \n",
      "12524                  1.962  \n",
      "12571                  1.930  \n",
      "12618                  1.721  \n",
      "12665                  1.587  \n",
      "12712                  1.557  \n",
      "12759                  1.410  \n",
      "12806                  1.438  \n",
      "12853                  1.525  \n",
      "12900                  1.538  \n",
      "12947                  1.524  \n",
      "12994                  1.418  \n",
      "13041                  1.333  \n",
      "13088                  1.429  \n",
      "13135                  1.478  \n",
      "13182                  1.517  \n",
      "13229                  1.610  \n",
      "13276                  1.671  \n",
      "13323                  1.668  \n",
      "13370                  1.722  \n",
      "13417                  1.679  \n",
      "13464                  1.563  \n",
      "13511                  1.687  \n",
      "13558                  1.653  \n",
      "13605                  1.633  \n",
      "13652                  1.570  \n",
      "13699                  1.565  \n",
      "13745                  1.343  \n",
      "13791                  1.080  \n",
      "13837                  1.231  \n",
      "...                      ...  \n",
      "21410                  1.177  \n",
      "21411                  2.147  \n",
      "21412                  0.300  \n",
      "21413                  3.522  \n",
      "21414                  1.656  \n",
      "21406                  1.765  \n",
      "21416                  1.533  \n",
      "21417                  2.043  \n",
      "21418                  0.352  \n",
      "21419                  2.933  \n",
      "21420                  1.788  \n",
      "21421                  2.616  \n",
      "21422                  1.229  \n",
      "21423                  1.657  \n",
      "21415                  2.696  \n",
      "21483                  1.572  \n",
      "21482                  0.718  \n",
      "21481                  2.740  \n",
      "21477                  1.698  \n",
      "21479                  1.239  \n",
      "21478                  1.934  \n",
      "21484                  1.816  \n",
      "21480                  1.716  \n",
      "21485                  1.667  \n",
      "21492                 -0.525  \n",
      "21489                  1.994  \n",
      "21491                  1.860  \n",
      "21493                  2.125  \n",
      "21494                  1.547  \n",
      "21495                  2.214  \n",
      "\n",
      "[5948 rows x 76 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, input_dim=76, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\carol\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\carol\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5948/5948 [==============================] - 1s 120us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 2/25\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 3/25\n",
      "5948/5948 [==============================] - 0s 70us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 4/25\n",
      "5948/5948 [==============================] - 1s 124us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 5/25\n",
      "5948/5948 [==============================] - 1s 106us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 6/25\n",
      "5948/5948 [==============================] - 1s 122us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 7/25\n",
      "5948/5948 [==============================] - 1s 87us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 8/25\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 9/25\n",
      "5948/5948 [==============================] - 1s 93us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 10/25\n",
      "5948/5948 [==============================] - 1s 98us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 11/25\n",
      "5948/5948 [==============================] - 1s 94us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 12/25\n",
      "5948/5948 [==============================] - 1s 99us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 13/25\n",
      "5948/5948 [==============================] - 1s 91us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 14/25\n",
      "5948/5948 [==============================] - 1s 85us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 15/25\n",
      "5948/5948 [==============================] - 0s 52us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 16/25\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 17/25\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 18/25\n",
      "5948/5948 [==============================] - 0s 69us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 19/25\n",
      "5948/5948 [==============================] - 1s 94us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 20/25\n",
      "5948/5948 [==============================] - 1s 87us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 21/25\n",
      "5948/5948 [==============================] - 1s 104us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 22/25\n",
      "5948/5948 [==============================] - 1s 117us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 23/25\n",
      "5948/5948 [==============================] - 1s 88us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 24/25\n",
      "5948/5948 [==============================] - 0s 78us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 25/25\n",
      "5948/5948 [==============================] - 1s 101us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "----------------------------------------------------------\n",
      "5948/5948 [==============================] - 1s 108us/step\n",
      "\n",
      "acc: 0.00%\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "--------------------------------------------\n",
      "7.115669132481506\n"
     ]
    }
   ],
   "source": [
    "#tutorial keras practice\n",
    "#https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "\n",
    "#####IGNORE THIS!!!!!!!!\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Softmax\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy\n",
    "\n",
    "model = Sequential()\n",
    "#parameters = number of neurons, initialization method, activation function\n",
    "model.add(Dense(32, input_dim=76, init = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(16, init = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_tra, y_tra, epochs=25, batch_size=32)\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "scores = model.evaluate(x_tra,y_tra)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "\n",
    "y_devpred = model.predict(x_dev)\n",
    "print(\"--------------------------------------------\")\n",
    "print(mean_squared_error(y_dev,y_devpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5948/5948 [==============================] - 2s 282us/step - loss: 23263.0275 - mean_squared_error: 23263.0275 - mean_absolute_error: 35.1115 - mean_absolute_percentage_error: 1009.8479 - cosine_proximity: -0.6449\n",
      "Epoch 2/25\n",
      "5948/5948 [==============================] - 1s 110us/step - loss: 0.3893 - mean_squared_error: 0.3893 - mean_absolute_error: 0.5268 - mean_absolute_percentage_error: 15.3620 - cosine_proximity: -1.0000\n",
      "Epoch 3/25\n",
      "5948/5948 [==============================] - 1s 120us/step - loss: 0.4579 - mean_squared_error: 0.4579 - mean_absolute_error: 0.5618 - mean_absolute_percentage_error: 16.3736 - cosine_proximity: -1.0000\n",
      "Epoch 4/25\n",
      "5948/5948 [==============================] - ETA: 0s - loss: 0.6201 - mean_squared_error: 0.6201 - mean_absolute_error: 0.6423 - mean_absolute_percentage_error: 18.6157 - cosine_proximity: -1.00 - 1s 134us/step - loss: 0.6225 - mean_squared_error: 0.6225 - mean_absolute_error: 0.6427 - mean_absolute_percentage_error: 18.6537 - cosine_proximity: -1.0000\n",
      "Epoch 5/25\n",
      "5948/5948 [==============================] - 0s 81us/step - loss: 0.6795 - mean_squared_error: 0.6795 - mean_absolute_error: 0.6651 - mean_absolute_percentage_error: 19.4146 - cosine_proximity: -1.0000\n",
      "Epoch 6/25\n",
      "5948/5948 [==============================] - 1s 140us/step - loss: 1.8318 - mean_squared_error: 1.8318 - mean_absolute_error: 1.0901 - mean_absolute_percentage_error: 31.1908 - cosine_proximity: -1.0000\n",
      "Epoch 7/25\n",
      "5948/5948 [==============================] - 1s 136us/step - loss: 32.1200 - mean_squared_error: 32.1200 - mean_absolute_error: 3.3054 - mean_absolute_percentage_error: 94.7710 - cosine_proximity: -0.7643\n",
      "Epoch 8/25\n",
      "5948/5948 [==============================] - 1s 136us/step - loss: 58.1845 - mean_squared_error: 58.1845 - mean_absolute_error: 4.9815 - mean_absolute_percentage_error: 142.3246 - cosine_proximity: -0.5911\n",
      "Epoch 9/25\n",
      "5948/5948 [==============================] - 1s 150us/step - loss: 273.9088 - mean_squared_error: 273.9088 - mean_absolute_error: 10.5116 - mean_absolute_percentage_error: 300.4383 - cosine_proximity: -0.3544 loss: 172.4000 - mean_squared_error: 172.4000 - mean_absolute_error: 12.9046 - mean_absolute_percentage_error: 366.9098 - cosine\n",
      "Epoch 10/25\n",
      "5948/5948 [==============================] - 1s 108us/step - loss: 18.0073 - mean_squared_error: 18.0073 - mean_absolute_error: 2.1366 - mean_absolute_percentage_error: 61.5638 - cosine_proximity: -0.8494\n",
      "Epoch 11/25\n",
      "5948/5948 [==============================] - 1s 117us/step - loss: 1.7043 - mean_squared_error: 1.7043 - mean_absolute_error: 1.0670 - mean_absolute_percentage_error: 30.6548 - cosine_proximity: -1.0000\n",
      "Epoch 12/25\n",
      "5948/5948 [==============================] - 1s 134us/step - loss: 58.2411 - mean_squared_error: 58.2411 - mean_absolute_error: 4.9197 - mean_absolute_percentage_error: 140.6706 - cosine_proximity: -0.5488\n",
      "Epoch 13/25\n",
      "5948/5948 [==============================] - 1s 129us/step - loss: 45.8171 - mean_squared_error: 45.8171 - mean_absolute_error: 4.5633 - mean_absolute_percentage_error: 130.4924 - cosine_proximity: -0.5481\n",
      "Epoch 14/25\n",
      "5948/5948 [==============================] - 1s 111us/step - loss: 103.8556 - mean_squared_error: 103.8556 - mean_absolute_error: 4.8766 - mean_absolute_percentage_error: 139.1868 - cosine_proximity: -0.7525\n",
      "Epoch 15/25\n",
      "5948/5948 [==============================] - 1s 118us/step - loss: 46.1165 - mean_squared_error: 46.1165 - mean_absolute_error: 4.5654 - mean_absolute_percentage_error: 130.8392 - cosine_proximity: -0.6019\n",
      "Epoch 16/25\n",
      "5948/5948 [==============================] - 1s 92us/step - loss: 62.9994 - mean_squared_error: 62.9994 - mean_absolute_error: 5.2540 - mean_absolute_percentage_error: 150.1204 - cosine_proximity: -0.4956\n",
      "Epoch 17/25\n",
      "5948/5948 [==============================] - 1s 114us/step - loss: 87.6599 - mean_squared_error: 87.6599 - mean_absolute_error: 6.6803 - mean_absolute_percentage_error: 191.0219 - cosine_proximity: -0.3867\n",
      "Epoch 18/25\n",
      "5948/5948 [==============================] - 1s 111us/step - loss: 37.6758 - mean_squared_error: 37.6758 - mean_absolute_error: 3.9439 - mean_absolute_percentage_error: 112.9538 - cosine_proximity: -0.6664\n",
      "Epoch 19/25\n",
      "5948/5948 [==============================] - 1s 116us/step - loss: 60.5727 - mean_squared_error: 60.5727 - mean_absolute_error: 4.2131 - mean_absolute_percentage_error: 120.8431 - cosine_proximity: -0.6785\n",
      "Epoch 20/25\n",
      "5948/5948 [==============================] - 1s 116us/step - loss: 41.5648 - mean_squared_error: 41.5648 - mean_absolute_error: 5.4829 - mean_absolute_percentage_error: 156.6915 - cosine_proximity: -0.3225\n",
      "Epoch 21/25\n",
      "5948/5948 [==============================] - 1s 108us/step - loss: 54.5870 - mean_squared_error: 54.5870 - mean_absolute_error: 6.2488 - mean_absolute_percentage_error: 179.0641 - cosine_proximity: -0.3114\n",
      "Epoch 22/25\n",
      "5948/5948 [==============================] - 0s 80us/step - loss: 52.6621 - mean_squared_error: 52.6621 - mean_absolute_error: 4.1975 - mean_absolute_percentage_error: 119.4897 - cosine_proximity: -0.6772\n",
      "Epoch 23/25\n",
      "5948/5948 [==============================] - 1s 103us/step - loss: 30.8205 - mean_squared_error: 30.8205 - mean_absolute_error: 4.4260 - mean_absolute_percentage_error: 126.5882 - cosine_proximity: -0.5377\n",
      "Epoch 24/25\n",
      "5948/5948 [==============================] - 1s 106us/step - loss: 36.1551 - mean_squared_error: 36.1551 - mean_absolute_error: 4.1584 - mean_absolute_percentage_error: 119.0147 - cosine_proximity: -0.5709\n",
      "Epoch 25/25\n",
      "5948/5948 [==============================] - 1s 113us/step - loss: 34.7718 - mean_squared_error: 34.7718 - mean_absolute_error: 4.4059 - mean_absolute_percentage_error: 126.3100 - cosine_proximity: -0.5269\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "1487/1487 [==============================] - 1s 369us/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-3ed13189074f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"----------------------------------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n%s: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#Regression Model:\n",
    "#1 Layer: 76 -> 12.78% and 15.28%\n",
    "#2 Layer: 76,1 -> 0.40% and 0.48%\n",
    "#3 Layers: 76, 32, 1 -> 0% and 65%\n",
    "#4 layers: 76,48,32,1 + adam +  -> 60.52% and 56.70%\n",
    "#4 Layers: 76,32,16,1 -> 61.33% and 57.18%\n",
    "#4 Layers: 76,32,8,1 -> 0%\n",
    "#4 layers: 76,48,8,1 -> 0%\n",
    "#6 layers: 76,48,32,16,8,1 -> 20% and 0%\n",
    "\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "#parameters = number of neurons, initialization method, activation function\n",
    "model.add(Dense(76, input_dim=76, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(48, kernel_initializer='normal',activation = 'relu'))\n",
    "model.add(Dense(32, kernel_initializer='normal',activation = 'relu'))\n",
    "model.add(Dense(16, kernel_initializer='normal',activation = 'relu'))\n",
    "#model.add(Dense(8, kernel_initializer='normal',activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "# Compile model\n",
    "#opt = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae', 'mape', 'cosine'])\n",
    "history = model.fit(np.asarray(x_tra), y_tra, epochs=25)\n",
    "\n",
    "pyplot.plot(history.history['mean_squared_error'])\n",
    "pyplot.plot(history.history['mean_absolute_error'])\n",
    "pyplot.plot(history.history['mean_absolute_percentage_error'])\n",
    "pyplot.plot(history.history['cosine_proximity'])\n",
    "pyplot.show()\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "scores = model.evaluate(np.asarray(x_dev),y_dev)\n",
    "for i in range[len(scores)]:\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[i], scores[i]*100))\n",
    "\n",
    "print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "scores = model.evaluate(np.asarray(x_test),y_test)\n",
    "for i in range[len(scores)]:\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[i], scores[i]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=76, init = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(16, init = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(8, init = 'uniform', activation = 'softmax'))\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=25, batch_size=32)\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "scores = model.evaluate(x_train,y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "y_devpred = model.predict(x_dev)\n",
    "print(\"--------------------------------------------\")\n",
    "print(mean_squared_error(y_dev,y_devpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Softmax\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape = (x_train.shape)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "y_devpred = model.predict(x_dev)\n",
    "print(\"--------------------------------------------\")\n",
    "print(mean_squared_error(y_dev,y_devpred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
