{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#parse data \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#label encoding on categorical data \n",
    "\n",
    "#FAMA 49CRSP Common Stocks \n",
    "df = pd.read_csv('ee6d2f60cdafb550.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 25 25 ... 22 10 47]\n",
      "       public_date  FFI49_desc  NFIRM  indret_ew  indret_vw  dpr_Median  \\\n",
      "12337     19930831          25      6   0.052904   0.068678       0.460   \n",
      "12383     19930930          25      6  -0.035253  -0.046242       0.460   \n",
      "12430     19931031          25      6   0.009319  -0.018318       0.460   \n",
      "12477     19931130          25      5  -0.022123  -0.014000       0.479   \n",
      "12524     19931231          25      5   0.096039   0.072342       0.479   \n",
      "12571     19940131          25      5   0.043779   0.009888       0.479   \n",
      "12618     19940228          25      6   0.012589   0.039537       0.394   \n",
      "12665     19940331          25      6  -0.012399  -0.052069       0.394   \n",
      "12712     19940430          25      6  -0.026779  -0.024747       0.394   \n",
      "12759     19940531          25      6  -0.001201   0.023906       0.394   \n",
      "12806     19940630          25      6  -0.001294  -0.037881       0.394   \n",
      "12853     19940731          25      6   0.023423   0.043118       0.394   \n",
      "12900     19940831          25      6   0.066949   0.055253       0.398   \n",
      "12947     19940930          25      6   0.004471  -0.026728       0.398   \n",
      "12994     19941031          25      6  -0.044276  -0.015566       0.398   \n",
      "13041     19941130          25      6  -0.040811  -0.035578       0.390   \n",
      "13088     19941231          25      6   0.007111   0.043045       0.390   \n",
      "13135     19950131          25      6   0.021079   0.023549       0.390   \n",
      "13182     19950228          25      6   0.058941   0.058359       0.422   \n",
      "13229     19950331          25      6   0.057380   0.023958       0.422   \n",
      "13276     19950430          25      6   0.062681   0.029973       0.422   \n",
      "13323     19950531          25      6   0.037249   0.031139       0.378   \n",
      "13370     19950630          25      6   0.056587   0.052070       0.378   \n",
      "13417     19950731          25      6   0.004115  -0.005009       0.378   \n",
      "13464     19950831          25      6  -0.000317   0.011758       0.422   \n",
      "13511     19950930          25      6   0.053875   0.015358       0.422   \n",
      "13558     19951031          25      6   0.004551  -0.003330       0.422   \n",
      "13605     19951130          25      6   0.047691   0.086727       0.409   \n",
      "13652     19951231          25      6   0.026102   0.035340       0.409   \n",
      "13699     19960131          25      6   0.044772   0.033413       0.409   \n",
      "...            ...         ...    ...        ...        ...         ...   \n",
      "25133     20161231          38      3   0.053255   0.054318       0.851   \n",
      "25134     20161231          39      3   0.023255   0.024929       0.437   \n",
      "25135     20161231          40     28  -0.009651   0.000257       0.000   \n",
      "25136     20161231          41      1  -0.036863  -0.036863       0.840   \n",
      "25137     20161231          42     15   0.020397   0.053427       0.291   \n",
      "25138     20161231          43      3  -0.066658  -0.075610       0.454   \n",
      "25139     20161231          44     15   0.007842   0.007611       0.294   \n",
      "25124     20161231          27      6  -0.004840  -0.002528       0.551   \n",
      "25132     20161231          35     34  -0.025950  -0.006974       0.297   \n",
      "25123     20161231          26     12  -0.002451  -0.008172       0.401   \n",
      "25110     20161231          12      6  -0.015227  -0.017383       0.087   \n",
      "25120     20161231          23     10   0.015469   0.015504       0.443   \n",
      "25141     20161231          46     33   0.036605   0.036090       0.641   \n",
      "25098     20161231           0      6   0.013718   0.016368       0.273   \n",
      "25100     20161231           2      5   0.037148   0.024898       0.232   \n",
      "25101     20161231           3     28   0.044543   0.044528       0.377   \n",
      "25102     20161231           4      4   0.010696   0.033009       0.425   \n",
      "25103     20161231           5      5  -0.004710  -0.006343       0.274   \n",
      "25106     20161231           8     17   0.001487   0.000991       0.316   \n",
      "25122     20161231          25     12   0.005462   0.005478       0.162   \n",
      "25107     20161231           9     12   0.008697   0.006098       0.484   \n",
      "25109     20161231          11      7  -0.050234  -0.012120       0.274   \n",
      "25111     20161231          14     16   0.003128   0.009955       0.375   \n",
      "25113     20161231          16     13   0.023076   0.036328       0.370   \n",
      "25114     20161231          17     14   0.046219   0.053876       0.616   \n",
      "25117     20161231          20      1  -0.057719  -0.057719       0.560   \n",
      "25118     20161231          21      6   0.018384   0.012505       0.510   \n",
      "25119     20161231          22      6  -0.012955   0.001926       0.026   \n",
      "25108     20161231          10     20   0.014731   0.029014       0.417   \n",
      "25142     20161231          47      9   0.006330   0.004922       0.392   \n",
      "\n",
      "       PEG_trailing_Median  bm_Median  CAPEI_Median  divyield_Median  \\\n",
      "12337                4.556      0.438        21.835           0.0229   \n",
      "12383                4.380      0.438        20.573           0.0245   \n",
      "12430                4.268      0.438        20.959           0.0251   \n",
      "12477                3.865      0.401        19.861           0.0273   \n",
      "12524                4.122      0.401        21.179           0.0255   \n",
      "12571                4.190      0.401        21.483           0.0212   \n",
      "12618               11.991      0.406        22.148           0.0234   \n",
      "12665               11.971      0.406        20.340           0.0233   \n",
      "12712               11.623      0.406        19.663           0.0251   \n",
      "12759                3.700      0.393        19.092           0.0245   \n",
      "12806                3.833      0.393        18.900           0.0252   \n",
      "12853                3.825      0.393        19.442           0.0248   \n",
      "12900                1.836      0.397        19.539           0.0239   \n",
      "12947                1.765      0.397        18.768           0.0239   \n",
      "12994                1.664      0.397        18.101           0.0244   \n",
      "13041                1.371      0.382        17.360           0.0269   \n",
      "13088                1.447      0.382        18.339           0.0271   \n",
      "13135                1.376      0.382        18.007           0.0260   \n",
      "13182                1.572      0.390        19.144           0.0249   \n",
      "13229                1.628      0.390        19.835           0.0246   \n",
      "13276                1.653      0.395        25.704           0.0237   \n",
      "13323                1.430      0.359        26.911           0.0223   \n",
      "13370                1.520      0.359        27.939           0.0212   \n",
      "13417                1.564      0.359        28.178           0.0217   \n",
      "13464                1.358      0.334        27.136           0.0214   \n",
      "13511                1.124      0.334        27.864           0.0212   \n",
      "13558                1.254      0.334        27.607           0.0216   \n",
      "13605                1.052      0.348        28.600           0.0204   \n",
      "13652                1.076      0.348        30.916           0.0197   \n",
      "13699                1.046      0.339        31.812           0.0174   \n",
      "...                    ...        ...           ...              ...   \n",
      "25133                5.987      0.273        28.409           0.0361   \n",
      "25134                0.997      0.166        24.194           0.0286   \n",
      "25135                1.724      0.177        27.234           0.0141   \n",
      "25136                3.066      0.523        33.958           0.0254   \n",
      "25137                0.765      0.502        22.358           0.0197   \n",
      "25138                0.611      0.233        25.309           0.0262   \n",
      "25139                1.132      0.432        21.286           0.0153   \n",
      "25124                1.497      0.101        31.726           0.0308   \n",
      "25132                1.589      0.305        21.762           0.0206   \n",
      "25123                0.807      0.312        16.528           0.0202   \n",
      "25110                0.500      0.655        14.286           0.0153   \n",
      "25120                3.701      0.212        26.668           0.0239   \n",
      "25141                2.104      0.798        23.026           0.0335   \n",
      "25098                1.627      0.241        22.032           0.0176   \n",
      "25100                0.446      0.542         9.784           0.0244   \n",
      "25101                2.449      0.877        15.996           0.0182   \n",
      "25102                8.059      0.185        26.059           0.0166   \n",
      "25103                0.568      0.293        41.512           0.0166   \n",
      "25106                1.826      0.219        32.407           0.0158   \n",
      "25122                2.457      0.265        23.877           0.0066   \n",
      "25107                1.148      0.295        24.150           0.0228   \n",
      "25109                0.845      0.210        19.595           0.0182   \n",
      "25111                1.298      0.178        24.618           0.0274   \n",
      "25113                1.429      0.596        22.645           0.0202   \n",
      "25114                1.332      0.219        25.145           0.0234   \n",
      "25117                2.528      0.033        23.666           0.0291   \n",
      "25118                2.234      0.411        18.186           0.0319   \n",
      "25119                1.222      0.396        21.419           0.0106   \n",
      "25108                1.664      0.262        24.592           0.0206   \n",
      "25142                3.060      0.244        22.458           0.0224   \n",
      "\n",
      "               ...            rect_turn_Median  sale_equity_Median  \\\n",
      "12337          ...                       5.494               3.146   \n",
      "12383          ...                       5.494               3.146   \n",
      "12430          ...                       5.685               3.027   \n",
      "12477          ...                       5.364               2.755   \n",
      "12524          ...                       5.364               2.755   \n",
      "12571          ...                       5.319               2.795   \n",
      "12618          ...                       4.789               2.451   \n",
      "12665          ...                       4.789               2.451   \n",
      "12712          ...                       4.789               2.525   \n",
      "12759          ...                       4.704               2.516   \n",
      "12806          ...                       4.704               2.516   \n",
      "12853          ...                       4.704               2.448   \n",
      "12900          ...                       4.589               2.449   \n",
      "12947          ...                       4.589               2.449   \n",
      "12994          ...                       4.589               2.513   \n",
      "13041          ...                       4.454               2.458   \n",
      "13088          ...                       4.454               2.458   \n",
      "13135          ...                       4.454               2.435   \n",
      "13182          ...                       4.443               2.568   \n",
      "13229          ...                       4.443               2.568   \n",
      "13276          ...                       4.443               2.526   \n",
      "13323          ...                       4.521               2.463   \n",
      "13370          ...                       4.521               2.463   \n",
      "13417          ...                       4.521               2.282   \n",
      "13464          ...                       4.551               2.364   \n",
      "13511          ...                       4.551               2.364   \n",
      "13558          ...                       4.551               2.510   \n",
      "13605          ...                       4.621               2.689   \n",
      "13652          ...                       4.621               2.689   \n",
      "13699          ...                       4.621               2.692   \n",
      "...            ...                         ...                 ...   \n",
      "25133          ...                      84.444               3.603   \n",
      "25134          ...                       9.870               1.646   \n",
      "25135          ...                       6.467               1.324   \n",
      "25136          ...                       9.640               2.084   \n",
      "25137          ...                       6.513               1.331   \n",
      "25138          ...                       5.333               2.850   \n",
      "25139          ...                      11.577               3.411   \n",
      "25124          ...                      24.406               3.634   \n",
      "25132          ...                      57.162               4.892   \n",
      "25123          ...                       4.811               1.908   \n",
      "25110          ...                      15.815               2.066   \n",
      "25120          ...                       7.705               3.133   \n",
      "25141          ...                       9.204               0.835   \n",
      "25098          ...                       6.365               2.672   \n",
      "25100          ...                       5.036               3.614   \n",
      "25101          ...                       0.092               0.422   \n",
      "25102          ...                       6.947               1.606   \n",
      "25103          ...                       7.212               1.779   \n",
      "25106          ...                       4.239               2.062   \n",
      "25122          ...                       6.038               1.086   \n",
      "25107          ...                       5.477               1.787   \n",
      "25109          ...                       9.522               2.378   \n",
      "25111          ...                       5.736               1.169   \n",
      "25113          ...                       4.244               0.499   \n",
      "25114          ...                      11.066               2.545   \n",
      "25117          ...                       5.158              15.531   \n",
      "25118          ...                       5.850               1.397   \n",
      "25119          ...                       7.180               1.817   \n",
      "25108          ...                       7.178               0.982   \n",
      "25142          ...                       9.550               4.712   \n",
      "\n",
      "       sale_invcap_Median  sale_nwc_Median  accrual_Median  rd_sale_Median  \\\n",
      "12337               2.601            9.285           0.049           0.068   \n",
      "12383               2.601            9.285           0.056           0.042   \n",
      "12430               2.484            9.285           0.056           0.042   \n",
      "12477               2.345            8.303           0.057           0.080   \n",
      "12524               2.345            8.303           0.057           0.080   \n",
      "12571               2.347            8.303           0.057           0.080   \n",
      "12618               2.121            7.341           0.038           0.068   \n",
      "12665               2.121            7.341           0.038           0.068   \n",
      "12712               2.163            7.321           0.038           0.068   \n",
      "12759               2.165            7.181           0.029           0.066   \n",
      "12806               2.165            7.181           0.029           0.066   \n",
      "12853               2.098            6.644           0.018           0.066   \n",
      "12900               2.110            6.244           0.001           0.064   \n",
      "12947               2.110            6.244           0.001           0.064   \n",
      "12994               2.162            6.483          -0.012           0.064   \n",
      "13041               2.132            6.226           0.012           0.063   \n",
      "13088               2.132            6.226           0.012           0.063   \n",
      "13135               2.121            6.171           0.012           0.063   \n",
      "13182               2.167            5.887           0.018           0.061   \n",
      "13229               2.167            5.887           0.018           0.061   \n",
      "13276               2.141            5.817           0.018           0.061   \n",
      "13323               2.157            5.438           0.016           0.059   \n",
      "13370               2.157            5.438           0.016           0.059   \n",
      "13417               2.032            5.562           0.016           0.059   \n",
      "13464               2.043            5.071           0.012           0.058   \n",
      "13511               2.043            5.071           0.012           0.058   \n",
      "13558               2.141            5.032           0.012           0.058   \n",
      "13605               2.123            5.813           0.007           0.056   \n",
      "13652               2.123            5.813           0.007           0.056   \n",
      "13699               2.116            5.819           0.007           0.056   \n",
      "...                   ...              ...             ...             ...   \n",
      "25133               1.202           37.073           0.000           0.010   \n",
      "25134               0.771            5.962           0.010           0.000   \n",
      "25135               0.848            2.536           0.092           0.142   \n",
      "25136               1.283            3.447           0.067           0.000   \n",
      "25137               0.642            5.943           0.029           0.000   \n",
      "25138               1.539            4.376           0.042           0.053   \n",
      "25139               1.975           22.973           0.054           0.000   \n",
      "25124               2.345           16.491           0.096           0.000   \n",
      "25132               2.979           11.434           0.064           0.000   \n",
      "25123               1.083            3.513           0.044           0.027   \n",
      "25110               1.686            9.334           0.022           0.000   \n",
      "25120               1.532           22.271           0.045           0.018   \n",
      "25141               0.412           28.583           0.048           0.000   \n",
      "25098               1.552            6.191           0.004           0.042   \n",
      "25100               1.527           10.102           0.052           0.038   \n",
      "25101               0.242            2.369           0.005           0.000   \n",
      "25102               0.768            4.789           0.023           0.000   \n",
      "25103               1.288            5.502           0.033           0.010   \n",
      "25106               0.837            8.357           0.043           0.000   \n",
      "25122               0.717            5.891           0.027           0.056   \n",
      "25107               0.819            5.277           0.045           0.028   \n",
      "25109               1.681            4.177           0.023           0.000   \n",
      "25111               0.771            2.366           0.035           0.174   \n",
      "25113               0.336            7.101           0.007           0.000   \n",
      "25114               1.555           12.893           0.039           0.007   \n",
      "25117               2.612           29.236           0.046           0.018   \n",
      "25118               0.955            2.551           0.032           0.130   \n",
      "25119               0.932           13.276           0.053           0.000   \n",
      "25108               0.838            2.092           0.052           0.099   \n",
      "25142               4.343            9.503           0.040           0.000   \n",
      "\n",
      "       adv_sale_Median  staff_sale_Median  PEG_1yrforward_Median  \\\n",
      "12337            0.013              0.000                 -0.333   \n",
      "12383            0.013              0.000                 -0.320   \n",
      "12430            0.013              0.000                 -0.350   \n",
      "12477            0.016              0.000                  0.677   \n",
      "12524            0.016              0.000                  0.722   \n",
      "12571            0.016              0.000                  0.814   \n",
      "12618            0.011              0.000                  1.108   \n",
      "12665            0.011              0.000                  1.013   \n",
      "12712            0.011              0.000                  0.897   \n",
      "12759            0.011              0.000                  0.816   \n",
      "12806            0.011              0.000                  0.866   \n",
      "12853            0.011              0.000                  1.522   \n",
      "12900            0.012              0.000                  1.517   \n",
      "12947            0.012              0.000                  1.456   \n",
      "12994            0.012              0.000                  0.806   \n",
      "13041            0.008              0.000                  0.923   \n",
      "13088            0.008              0.000                  0.975   \n",
      "13135            0.008              0.000                  1.063   \n",
      "13182            0.000              0.000                  1.046   \n",
      "13229            0.000              0.000                  1.123   \n",
      "13276            0.000              0.000                  1.136   \n",
      "13323            0.000              0.000                  1.055   \n",
      "13370            0.000              0.000                  1.134   \n",
      "13417            0.000              0.000                  1.098   \n",
      "13464            0.000              0.000                  1.022   \n",
      "13511            0.000              0.000                  1.002   \n",
      "13558            0.000              0.000                  0.935   \n",
      "13605            0.000              0.000                  0.873   \n",
      "13652            0.000              0.000                  0.863   \n",
      "13699            0.000              0.000                  0.840   \n",
      "...                ...                ...                    ...   \n",
      "25133            0.013              0.000                  3.095   \n",
      "25134            0.078              0.000                  0.567   \n",
      "25135            0.017              0.000                  1.762   \n",
      "25136            0.000              0.000                  1.132   \n",
      "25137            0.040              0.000                  0.458   \n",
      "25138            0.092              0.000                  8.788   \n",
      "25139            0.000              0.245                  2.168   \n",
      "25124            0.022              0.087                  1.605   \n",
      "25132            0.011              0.000                  1.195   \n",
      "25123            0.000              0.000                 -0.713   \n",
      "25110            0.002              0.000                  0.931   \n",
      "25120            0.056              0.000                  3.005   \n",
      "25141            0.000              0.000                  2.240   \n",
      "25098            0.000              0.000                  2.150   \n",
      "25100            0.023              0.000                  0.164   \n",
      "25101            0.015              0.306                  1.301   \n",
      "25102            0.092              0.000                  2.981   \n",
      "25103            0.000              0.000                  0.718   \n",
      "25106            0.000              0.000                  2.369   \n",
      "25122            0.000              0.000                  2.091   \n",
      "25107            0.000              0.000                  1.718   \n",
      "25109            0.047              0.000                  2.371   \n",
      "25111            0.014              0.000                  1.328   \n",
      "25113            0.000              0.242                  1.433   \n",
      "25114            0.029              0.000                  2.894   \n",
      "25117            0.000              0.000                  2.765   \n",
      "25118            0.004              0.000                  1.552   \n",
      "25119            0.000              0.228                  1.508   \n",
      "25108            0.000              0.000                  1.713   \n",
      "25142            0.000              0.000                 -4.237   \n",
      "\n",
      "       PEG_ltgforward_Median  \n",
      "12337                  1.572  \n",
      "12383                  1.445  \n",
      "12430                  1.422  \n",
      "12477                  1.848  \n",
      "12524                  1.962  \n",
      "12571                  1.930  \n",
      "12618                  1.721  \n",
      "12665                  1.587  \n",
      "12712                  1.557  \n",
      "12759                  1.410  \n",
      "12806                  1.438  \n",
      "12853                  1.525  \n",
      "12900                  1.538  \n",
      "12947                  1.524  \n",
      "12994                  1.418  \n",
      "13041                  1.333  \n",
      "13088                  1.429  \n",
      "13135                  1.478  \n",
      "13182                  1.517  \n",
      "13229                  1.610  \n",
      "13276                  1.671  \n",
      "13323                  1.668  \n",
      "13370                  1.722  \n",
      "13417                  1.679  \n",
      "13464                  1.563  \n",
      "13511                  1.687  \n",
      "13558                  1.653  \n",
      "13605                  1.633  \n",
      "13652                  1.570  \n",
      "13699                  1.565  \n",
      "...                      ...  \n",
      "25133                  3.081  \n",
      "25134                  1.997  \n",
      "25135                  2.425  \n",
      "25136                  1.401  \n",
      "25137                  1.019  \n",
      "25138                  2.122  \n",
      "25139                  2.081  \n",
      "25124                  1.987  \n",
      "25132                  1.778  \n",
      "25123                  2.365  \n",
      "25110                  1.580  \n",
      "25120                  3.063  \n",
      "25141                  3.676  \n",
      "25098                  2.645  \n",
      "25100                 -1.294  \n",
      "25101                  1.945  \n",
      "25102                  3.205  \n",
      "25103                  1.579  \n",
      "25106                  2.179  \n",
      "25122                  2.545  \n",
      "25107                  2.572  \n",
      "25109                  2.236  \n",
      "25111                  2.379  \n",
      "25113                  1.546  \n",
      "25114                  3.445  \n",
      "25117                  2.721  \n",
      "25118                  1.875  \n",
      "25119                  1.884  \n",
      "25108                  2.218  \n",
      "25142                  2.072  \n",
      "\n",
      "[9297 rows x 76 columns]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing \n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#preprocessing here\n",
    "#sort by date \n",
    "df = df.sort_values(by = 'public_date', ascending = True)\n",
    "\n",
    "#encode integer categories into numbers \n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df.FFI49_desc)\n",
    "print(integer_encoded)\n",
    "df.FFI49_desc = integer_encoded\n",
    "df.divyield_Median = [float(x.strip('%'))/100 for x in df.divyield_Median]\n",
    "print(df)\n",
    "\n",
    "#todo: https://www.gsb.stanford.edu/library/articles/databases/links/financial-ratios-suite?fbclid=IwAR0EGNGk9DdxQjEHfdaoUhdY3tNzAWDogYDzuuJi1zT_muL-uJtWQw19Fzk\n",
    "\n",
    "#get output first \n",
    "ewlabels = df.indret_ew\n",
    "vwlabels = df.indret_vw\n",
    "\n",
    "#3year on year change as a prediction feature, raw pct change \n",
    "yoythree = ewlabels.diff(periods = 3)\n",
    "#3 years rolling percent change, averaged ie. (y1-y2 + (y3-y2)change)/2 \n",
    "rollavgpct = ewlabels.rolling(3).mean()\n",
    "\n",
    "#drop first 3 years\n",
    "df = df.iloc[3:]\n",
    "ewlabels = ewlabels.iloc[3:]\n",
    "yoythree = yoythree.iloc[3:]\n",
    "#yoypctthree = yoypctthree.iloc[3:]\n",
    "rollavgpct = rollavgpct.iloc[3:]\n",
    "\n",
    "#add -1 and 1 so the bins will take on bins to be equal and set to max -1 and 1\n",
    "extrema = pd.Series([-1,1])\n",
    "ewnlabels = ewlabels.append(extrema)\n",
    "\n",
    "#make a new output (bucket by percentage?)\n",
    "enc = KBinsDiscretizer(n_bins=8, encode='ordinal',strategy = 'uniform')\n",
    "ewnlabels = np.asarray(ewnlabels)\n",
    "ewnlabels = ewnlabels.reshape((-1,1))\n",
    "labels_binned = enc.fit_transform(ewnlabels)\n",
    "\n",
    "labels_binned = labels_binned[:-2]\n",
    "\n",
    "#1 Split-Timer series data, 0.64 Train, 0.16 dev, 0.2 Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, labels_binned, test_size = 0.2, shuffle = False)\n",
    "x_tra, x_dev, y_tra, y_dev = train_test_split(x_train, y_train, test_size = 0.2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12477    19931130\n",
      "12524    19931231\n",
      "12571    19940131\n",
      "12618    19940228\n",
      "12665    19940331\n",
      "12712    19940430\n",
      "12759    19940531\n",
      "12806    19940630\n",
      "12853    19940731\n",
      "12900    19940831\n",
      "12947    19940930\n",
      "12994    19941031\n",
      "13041    19941130\n",
      "13088    19941231\n",
      "13135    19950131\n",
      "13182    19950228\n",
      "13229    19950331\n",
      "13276    19950430\n",
      "13323    19950531\n",
      "13370    19950630\n",
      "13417    19950731\n",
      "13464    19950831\n",
      "13511    19950930\n",
      "13558    19951031\n",
      "13605    19951130\n",
      "13652    19951231\n",
      "13699    19960131\n",
      "13745    19960229\n",
      "13791    19960331\n",
      "13837    19960430\n",
      "           ...   \n",
      "21410    20100331\n",
      "21411    20100331\n",
      "21412    20100331\n",
      "21413    20100331\n",
      "21414    20100331\n",
      "21406    20100331\n",
      "21416    20100331\n",
      "21417    20100331\n",
      "21418    20100331\n",
      "21419    20100331\n",
      "21420    20100331\n",
      "21421    20100331\n",
      "21422    20100331\n",
      "21423    20100331\n",
      "21415    20100331\n",
      "21483    20100430\n",
      "21482    20100430\n",
      "21481    20100430\n",
      "21477    20100430\n",
      "21479    20100430\n",
      "21478    20100430\n",
      "21484    20100430\n",
      "21480    20100430\n",
      "21485    20100430\n",
      "21492    20100430\n",
      "21489    20100430\n",
      "21491    20100430\n",
      "21493    20100430\n",
      "21494    20100430\n",
      "21495    20100430\n",
      "Name: public_date, Length: 5948, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(x_tra['public_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "fig1 = pyplot.figure(1, figsize = (6,6))\n",
    "pyplot.plot(x_tra['public_date'], y_tra, color = 'green', label = 'industry_ew_train')\n",
    "pyplot.plot(x_dev['public_date'], y_dev, color = 'yellow', label = 'industry_ew_dev')\n",
    "pyplot.plot(x_test['public_date'], y_test, color = 'red', label = 'industry_ew_test')\n",
    "pyplot.xlabel('Date')\n",
    "pyplot.ylabel('Industry Return (Equally Weighted)')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "1487\n",
      "1859\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(x_train.shape[1])\n",
    "print(x_dev.shape[0])\n",
    "print(x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5948/5948 [==============================] - 1s 155us/step - loss: 1.9327 - acc: 0.4117\n",
      "Epoch 2/100\n",
      "5948/5948 [==============================] - 1s 91us/step - loss: 1.4869 - acc: 0.4235\n",
      "Epoch 3/100\n",
      "5948/5948 [==============================] - 0s 76us/step - loss: 1.1901 - acc: 0.4235\n",
      "Epoch 4/100\n",
      "5948/5948 [==============================] - 1s 93us/step - loss: 1.0023 - acc: 0.5032\n",
      "Epoch 5/100\n",
      "5948/5948 [==============================] - 0s 81us/step - loss: 0.8921 - acc: 0.5642\n",
      "Epoch 6/100\n",
      "5948/5948 [==============================] - 1s 88us/step - loss: 0.8375 - acc: 0.5642\n",
      "Epoch 7/100\n",
      "5948/5948 [==============================] - 0s 75us/step - loss: 0.8101 - acc: 0.5642\n",
      "Epoch 8/100\n",
      "5948/5948 [==============================] - 1s 90us/step - loss: 0.7945 - acc: 0.5642\n",
      "Epoch 9/100\n",
      "5948/5948 [==============================] - 0s 78us/step - loss: 0.7851 - acc: 0.5642\n",
      "Epoch 10/100\n",
      "5948/5948 [==============================] - 1s 90us/step - loss: 0.7786 - acc: 0.5642\n",
      "Epoch 11/100\n",
      "5948/5948 [==============================] - 1s 87us/step - loss: 0.7740 - acc: 0.5642\n",
      "Epoch 12/100\n",
      "5948/5948 [==============================] - 1s 90us/step - loss: 0.7705 - acc: 0.5642\n",
      "Epoch 13/100\n",
      "5948/5948 [==============================] - 0s 79us/step - loss: 0.7677 - acc: 0.5642\n",
      "Epoch 14/100\n",
      "5948/5948 [==============================] - 1s 87us/step - loss: 0.7655 - acc: 0.5642\n",
      "Epoch 15/100\n",
      "5948/5948 [==============================] - 0s 75us/step - loss: 0.7637 - acc: 0.5642\n",
      "Epoch 16/100\n",
      "5948/5948 [==============================] - 1s 86us/step - loss: 0.7622 - acc: 0.5642\n",
      "Epoch 17/100\n",
      "5948/5948 [==============================] - 0s 76us/step - loss: 0.7610 - acc: 0.5642\n",
      "Epoch 18/100\n",
      "5948/5948 [==============================] - 1s 89us/step - loss: 0.7598 - acc: 0.5642\n",
      "Epoch 19/100\n",
      "5948/5948 [==============================] - 0s 80us/step - loss: 0.7589 - acc: 0.5642\n",
      "Epoch 20/100\n",
      "5948/5948 [==============================] - 0s 84us/step - loss: 0.7581 - acc: 0.5642\n",
      "Epoch 21/100\n",
      "5948/5948 [==============================] - 0s 80us/step - loss: 0.7574 - acc: 0.5642\n",
      "Epoch 22/100\n",
      "5948/5948 [==============================] - 1s 87us/step - loss: 0.7567 - acc: 0.5642\n",
      "Epoch 23/100\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7562 - acc: 0.5642\n",
      "Epoch 24/100\n",
      "5948/5948 [==============================] - 0s 81us/step - loss: 0.7557 - acc: 0.5642\n",
      "Epoch 25/100\n",
      "5948/5948 [==============================] - 0s 72us/step - loss: 0.7552 - acc: 0.5642\n",
      "Epoch 26/100\n",
      "5948/5948 [==============================] - 1s 85us/step - loss: 0.7548 - acc: 0.5642\n",
      "Epoch 27/100\n",
      "5948/5948 [==============================] - 1s 107us/step - loss: 0.7545 - acc: 0.5642 0s - loss: 0.7546 - acc: 0.564\n",
      "Epoch 28/100\n",
      "5948/5948 [==============================] - 1s 109us/step - loss: 0.7541 - acc: 0.5642\n",
      "Epoch 29/100\n",
      "5948/5948 [==============================] - 1s 109us/step - loss: 0.7538 - acc: 0.5642\n",
      "Epoch 30/100\n",
      "5948/5948 [==============================] - 1s 101us/step - loss: 0.7536 - acc: 0.5642\n",
      "Epoch 31/100\n",
      "5948/5948 [==============================] - 0s 83us/step - loss: 0.7533 - acc: 0.5642\n",
      "Epoch 32/100\n",
      "5948/5948 [==============================] - 1s 108us/step - loss: 0.7531 - acc: 0.5642\n",
      "Epoch 33/100\n",
      "5948/5948 [==============================] - 1s 96us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 34/100\n",
      "5948/5948 [==============================] - 1s 99us/step - loss: 0.7527 - acc: 0.5642\n",
      "Epoch 35/100\n",
      "5948/5948 [==============================] - 0s 81us/step - loss: 0.7525 - acc: 0.5642\n",
      "Epoch 36/100\n",
      "5948/5948 [==============================] - 0s 84us/step - loss: 0.7524 - acc: 0.5642\n",
      "Epoch 37/100\n",
      "5948/5948 [==============================] - 0s 76us/step - loss: 0.7522 - acc: 0.5642\n",
      "Epoch 38/100\n",
      "5948/5948 [==============================] - 0s 83us/step - loss: 0.7521 - acc: 0.5642\n",
      "Epoch 39/100\n",
      "5948/5948 [==============================] - 0s 79us/step - loss: 0.7521 - acc: 0.5642\n",
      "Epoch 40/100\n",
      "5948/5948 [==============================] - 0s 74us/step - loss: 0.7519 - acc: 0.5642\n",
      "Epoch 41/100\n",
      "5948/5948 [==============================] - 0s 77us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 42/100\n",
      "5948/5948 [==============================] - 0s 75us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 43/100\n",
      "5948/5948 [==============================] - 0s 83us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 44/100\n",
      "5948/5948 [==============================] - 0s 76us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 45/100\n",
      "5948/5948 [==============================] - 0s 83us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 46/100\n",
      "5948/5948 [==============================] - 0s 71us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 47/100\n",
      "5948/5948 [==============================] - 0s 77us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 48/100\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 49/100\n",
      "5948/5948 [==============================] - 0s 77us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 50/100\n",
      "5948/5948 [==============================] - 0s 78us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 51/100\n",
      "5948/5948 [==============================] - 0s 81us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 52/100\n",
      "5948/5948 [==============================] - 0s 77us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 53/100\n",
      "5948/5948 [==============================] - 0s 78us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 54/100\n",
      "5948/5948 [==============================] - 0s 78us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 55/100\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 56/100\n",
      "5948/5948 [==============================] - 0s 74us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 57/100\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 58/100\n",
      "5948/5948 [==============================] - 0s 75us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 59/100\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 60/100\n",
      "5948/5948 [==============================] - 0s 78us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 61/100\n",
      "5948/5948 [==============================] - 0s 79us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 62/100\n",
      "5948/5948 [==============================] - 0s 77us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 63/100\n",
      "5948/5948 [==============================] - 0s 75us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 64/100\n",
      "5948/5948 [==============================] - 0s 74us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 65/100\n",
      "5948/5948 [==============================] - 0s 76us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 66/100\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 67/100\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 68/100\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 69/100\n",
      "5948/5948 [==============================] - 0s 75us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 70/100\n",
      "5948/5948 [==============================] - 0s 78us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 71/100\n",
      "5948/5948 [==============================] - 0s 80us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 72/100\n",
      "5948/5948 [==============================] - 0s 77us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 73/100\n",
      "5948/5948 [==============================] - 0s 80us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 74/100\n",
      "5948/5948 [==============================] - 1s 96us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 75/100\n",
      "5948/5948 [==============================] - 1s 87us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 76/100\n",
      "5948/5948 [==============================] - 0s 81us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 77/100\n",
      "5948/5948 [==============================] - 0s 79us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 78/100\n",
      "5948/5948 [==============================] - 0s 74us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 79/100\n",
      "5948/5948 [==============================] - 0s 81us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 80/100\n",
      "5948/5948 [==============================] - 1s 94us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 81/100\n",
      "5948/5948 [==============================] - 1s 84us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 82/100\n",
      "5948/5948 [==============================] - 1s 85us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 83/100\n",
      "5948/5948 [==============================] - 1s 121us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 84/100\n",
      "5948/5948 [==============================] - 0s 82us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 85/100\n",
      "5948/5948 [==============================] - 1s 96us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 86/100\n",
      "5948/5948 [==============================] - 1s 94us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 87/100\n",
      "5948/5948 [==============================] - 1s 90us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 88/100\n",
      "5948/5948 [==============================] - 1s 87us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 89/100\n",
      "5948/5948 [==============================] - 1s 89us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 90/100\n",
      "5948/5948 [==============================] - 1s 99us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 91/100\n",
      "5948/5948 [==============================] - 1s 86us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 92/100\n",
      "5948/5948 [==============================] - 1s 89us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 93/100\n",
      "5948/5948 [==============================] - 0s 81us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 94/100\n",
      "5948/5948 [==============================] - 1s 87us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 95/100\n",
      "5948/5948 [==============================] - 0s 82us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 96/100\n",
      "5948/5948 [==============================] - 1s 89us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 97/100\n",
      "5948/5948 [==============================] - 1s 120us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 98/100\n",
      "5948/5948 [==============================] - 1s 121us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 99/100\n",
      "5948/5948 [==============================] - 1s 88us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 100/100\n",
      "5948/5948 [==============================] - 0s 81us/step - loss: 0.7507 - acc: 0.5642\n",
      "1487/1487 [==============================] - 0s 121us/step\n",
      "\n",
      "loss: 69.74%\n",
      "\n",
      "acc: 58.69%\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "1859/1859 [==============================] - 0s 61us/step\n",
      "\n",
      "loss: 69.76%\n",
      "\n",
      "acc: 58.69%\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(76, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(60, activation=tf.nn.softmax),\n",
    "    keras.layers.Dense(40, activation=tf.nn.softmax),\n",
    "    keras.layers.Dense(20, activation=tf.nn.softmax),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    keras.layers.Dense(9, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.asarray(x_tra), y_tra, epochs=100)\n",
    "test_loss, test_acc = model.evaluate(np.asarray(x_dev), y_dev)\n",
    "for i in range(len(scores)):\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[i], scores[i]*100))\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "test_predictions = model.predict(x_test) \n",
    "scores = model.evaluate(np.asarray(x_test),y_test)\n",
    "for i in range(len(scores)):\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[i], scores[i]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5948/5948 [==============================] - 1s 208us/step - loss: 1.9342 - acc: 0.3362\n",
      "Epoch 2/100\n",
      "5948/5948 [==============================] - 1s 133us/step - loss: 1.4674 - acc: 0.4235\n",
      "Epoch 3/100\n",
      "5948/5948 [==============================] - 1s 161us/step - loss: 1.1741 - acc: 0.4235\n",
      "Epoch 4/100\n",
      "5948/5948 [==============================] - 1s 143us/step - loss: 1.0065 - acc: 0.5493\n",
      "Epoch 5/100\n",
      "5948/5948 [==============================] - 1s 107us/step - loss: 0.9130 - acc: 0.5642\n",
      "Epoch 6/100\n",
      "5948/5948 [==============================] - 0s 62us/step - loss: 0.8600 - acc: 0.5642\n",
      "Epoch 7/100\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.8285 - acc: 0.5642\n",
      "Epoch 8/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.8084 - acc: 0.5642\n",
      "Epoch 9/100\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7947 - acc: 0.5642\n",
      "Epoch 10/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7852 - acc: 0.5642\n",
      "Epoch 11/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7783 - acc: 0.5642\n",
      "Epoch 12/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7732 - acc: 0.5642\n",
      "Epoch 13/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7695 - acc: 0.5642\n",
      "Epoch 14/100\n",
      "5948/5948 [==============================] - 0s 66us/step - loss: 0.7666 - acc: 0.5642\n",
      "Epoch 15/100\n",
      "5948/5948 [==============================] - 0s 70us/step - loss: 0.7642 - acc: 0.5642\n",
      "Epoch 16/100\n",
      "5948/5948 [==============================] - 0s 67us/step - loss: 0.7624 - acc: 0.5642\n",
      "Epoch 17/100\n",
      "5948/5948 [==============================] - 0s 70us/step - loss: 0.7609 - acc: 0.5642\n",
      "Epoch 18/100\n",
      "5948/5948 [==============================] - 1s 141us/step - loss: 0.7597 - acc: 0.5642\n",
      "Epoch 19/100\n",
      "5948/5948 [==============================] - 0s 83us/step - loss: 0.7588 - acc: 0.5642\n",
      "Epoch 20/100\n",
      "5948/5948 [==============================] - 1s 85us/step - loss: 0.7578 - acc: 0.5642\n",
      "Epoch 21/100\n",
      "5948/5948 [==============================] - 0s 83us/step - loss: 0.7571 - acc: 0.5642\n",
      "Epoch 22/100\n",
      "5948/5948 [==============================] - 1s 98us/step - loss: 0.7564 - acc: 0.5642\n",
      "Epoch 23/100\n",
      "5948/5948 [==============================] - 1s 127us/step - loss: 0.7558 - acc: 0.5642\n",
      "Epoch 24/100\n",
      "5948/5948 [==============================] - 1s 132us/step - loss: 0.7552 - acc: 0.5642\n",
      "Epoch 25/100\n",
      "5948/5948 [==============================] - 1s 158us/step - loss: 0.7549 - acc: 0.5642\n",
      "Epoch 26/100\n",
      "5948/5948 [==============================] - 1s 87us/step - loss: 0.7545 - acc: 0.5642\n",
      "Epoch 27/100\n",
      "5948/5948 [==============================] - 0s 83us/step - loss: 0.7542 - acc: 0.5642\n",
      "Epoch 28/100\n",
      "5948/5948 [==============================] - 0s 71us/step - loss: 0.7537 - acc: 0.5642\n",
      "Epoch 29/100\n",
      "5948/5948 [==============================] - 0s 63us/step - loss: 0.7535 - acc: 0.5642\n",
      "Epoch 30/100\n",
      "5948/5948 [==============================] - 1s 101us/step - loss: 0.7533 - acc: 0.5642\n",
      "Epoch 31/100\n",
      "5948/5948 [==============================] - 1s 98us/step - loss: 0.7530 - acc: 0.5642\n",
      "Epoch 32/100\n",
      "5948/5948 [==============================] - 0s 63us/step - loss: 0.7528 - acc: 0.5642\n",
      "Epoch 33/100\n",
      "5948/5948 [==============================] - 1s 91us/step - loss: 0.7527 - acc: 0.5642\n",
      "Epoch 34/100\n",
      "5948/5948 [==============================] - 0s 82us/step - loss: 0.7525 - acc: 0.5642\n",
      "Epoch 35/100\n",
      "5948/5948 [==============================] - 0s 72us/step - loss: 0.7524 - acc: 0.5642\n",
      "Epoch 36/100\n",
      "5948/5948 [==============================] - 0s 69us/step - loss: 0.7521 - acc: 0.5642\n",
      "Epoch 37/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7521 - acc: 0.5642\n",
      "Epoch 38/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7520 - acc: 0.5642\n",
      "Epoch 39/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 40/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 41/100\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 42/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 43/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 44/100\n",
      "5948/5948 [==============================] - 0s 78us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 45/100\n",
      "5948/5948 [==============================] - 1s 95us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 46/100\n",
      "5948/5948 [==============================] - 1s 160us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 47/100\n",
      "5948/5948 [==============================] - 1s 129us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 48/100\n",
      "5948/5948 [==============================] - 1s 126us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 49/100\n",
      "5948/5948 [==============================] - 1s 86us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 50/100\n",
      "5948/5948 [==============================] - 0s 65us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 51/100\n",
      "5948/5948 [==============================] - 0s 59us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 52/100\n",
      "5948/5948 [==============================] - 1s 110us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 53/100\n",
      "5948/5948 [==============================] - 0s 83us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 54/100\n",
      "5948/5948 [==============================] - 0s 74us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 55/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 56/100\n",
      "5948/5948 [==============================] - 1s 95us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 57/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 58/100\n",
      "5948/5948 [==============================] - 0s 63us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 59/100\n",
      "5948/5948 [==============================] - 0s 74us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 60/100\n",
      "5948/5948 [==============================] - 0s 48us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 61/100\n",
      "5948/5948 [==============================] - 0s 47us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 62/100\n",
      "5948/5948 [==============================] - 0s 48us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 63/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 64/100\n",
      "5948/5948 [==============================] - 0s 77us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 65/100\n",
      "5948/5948 [==============================] - 1s 88us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 66/100\n",
      "5948/5948 [==============================] - 1s 94us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 67/100\n",
      "5948/5948 [==============================] - 1s 124us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 68/100\n",
      "5948/5948 [==============================] - 1s 150us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 69/100\n",
      "5948/5948 [==============================] - 1s 148us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 70/100\n",
      "5948/5948 [==============================] - 0s 79us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 71/100\n",
      "5948/5948 [==============================] - 1s 120us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 72/100\n",
      "5948/5948 [==============================] - 0s 84us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 73/100\n",
      "5948/5948 [==============================] - 0s 68us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 74/100\n",
      "5948/5948 [==============================] - 0s 59us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 75/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 76/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 77/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 78/100\n",
      "5948/5948 [==============================] - 0s 62us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 79/100\n",
      "5948/5948 [==============================] - 0s 60us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 80/100\n",
      "5948/5948 [==============================] - 0s 74us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 81/100\n",
      "5948/5948 [==============================] - 0s 60us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 82/100\n",
      "5948/5948 [==============================] - 0s 72us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 83/100\n",
      "5948/5948 [==============================] - 0s 59us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 84/100\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 85/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 86/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 87/100\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 88/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 89/100\n",
      "5948/5948 [==============================] - 0s 67us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 90/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 91/100\n",
      "5948/5948 [==============================] - 1s 93us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 92/100\n",
      "5948/5948 [==============================] - 1s 104us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 93/100\n",
      "5948/5948 [==============================] - 1s 134us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 94/100\n",
      "5948/5948 [==============================] - 1s 115us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 95/100\n",
      "5948/5948 [==============================] - 0s 75us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 96/100\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 97/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 98/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 99/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 100/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7508 - acc: 0.5642\n",
      "1487/1487 [==============================] - 0s 101us/step\n",
      "Epoch 1/100\n",
      "5948/5948 [==============================] - 1s 106us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 2/100\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 3/100\n",
      "5948/5948 [==============================] - 0s 63us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 4/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 5/100\n",
      "5948/5948 [==============================] - 0s 61us/step - loss: 0.7519 - acc: 0.5642\n",
      "Epoch 6/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7509 - acc: 0.5627\n",
      "Epoch 7/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 8/100\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 9/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 10/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 11/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 12/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 13/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 14/100\n",
      "5948/5948 [==============================] - 0s 52us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 15/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 16/100\n",
      "5948/5948 [==============================] - 0s 59us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 17/100\n",
      "5948/5948 [==============================] - 1s 98us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 18/100\n",
      "5948/5948 [==============================] - 1s 113us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 19/100\n",
      "5948/5948 [==============================] - 1s 105us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 20/100\n",
      "5948/5948 [==============================] - 1s 89us/step - loss: 0.7521 - acc: 0.5642\n",
      "Epoch 21/100\n",
      "5948/5948 [==============================] - 0s 64us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 22/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 23/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 24/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 25/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 26/100\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 27/100\n",
      "5948/5948 [==============================] - 0s 62us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 28/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 29/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 30/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 31/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 32/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 33/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 34/100\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 35/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 36/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 37/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 38/100\n",
      "5948/5948 [==============================] - 0s 60us/step - loss: 0.7519 - acc: 0.5642\n",
      "Epoch 39/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 40/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 41/100\n",
      "5948/5948 [==============================] - 0s 48us/step - loss: 0.7520 - acc: 0.5642\n",
      "Epoch 42/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 43/100\n",
      "5948/5948 [==============================] - 0s 64us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 44/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 45/100\n",
      "5948/5948 [==============================] - 1s 96us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 46/100\n",
      "5948/5948 [==============================] - 1s 149us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 47/100\n",
      "5948/5948 [==============================] - 1s 97us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 48/100\n",
      "5948/5948 [==============================] - 1s 85us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 49/100\n",
      "5948/5948 [==============================] - 0s 66us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 50/100\n",
      "5948/5948 [==============================] - 0s 66us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 51/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 52/100\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 53/100\n",
      "5948/5948 [==============================] - 0s 60us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 54/100\n",
      "5948/5948 [==============================] - 0s 60us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 55/100\n",
      "5948/5948 [==============================] - 0s 62us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 56/100\n",
      "5948/5948 [==============================] - 0s 59us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 57/100\n",
      "5948/5948 [==============================] - 0s 74us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 58/100\n",
      "5948/5948 [==============================] - 0s 62us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 59/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 60/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 61/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 62/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 64/100\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 65/100\n",
      "5948/5948 [==============================] - 0s 48us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 66/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 67/100\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 68/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 69/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 70/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 71/100\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 72/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 73/100\n",
      "5948/5948 [==============================] - 1s 114us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 74/100\n",
      "5948/5948 [==============================] - 1s 129us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 75/100\n",
      "5948/5948 [==============================] - 1s 94us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 76/100\n",
      "5948/5948 [==============================] - 1s 88us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 77/100\n",
      "5948/5948 [==============================] - 0s 59us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 78/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 79/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 80/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 81/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 82/100\n",
      "5948/5948 [==============================] - 0s 52us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 83/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 84/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 85/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 86/100\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7521 - acc: 0.5642\n",
      "Epoch 87/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 88/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 89/100\n",
      "5948/5948 [==============================] - 0s 47us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 90/100\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7506 - acc: 0.5642\n",
      "Epoch 91/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 92/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 93/100\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 94/100\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 95/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 96/100\n",
      "5948/5948 [==============================] - 0s 68us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 97/100\n",
      "5948/5948 [==============================] - 0s 60us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 98/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 99/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 100/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7517 - acc: 0.5642\n",
      "1487/1487 [==============================] - 0s 233us/step\n",
      "Epoch 1/100\n",
      "5948/5948 [==============================] - 1s 250us/step - loss: 0.7515 - acc: 0.5632\n",
      "Epoch 2/100\n",
      "5948/5948 [==============================] - 0s 65us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 3/100\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 4/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 5/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 6/100\n",
      "5948/5948 [==============================] - 0s 60us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 7/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 8/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 9/100\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 10/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 11/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 12/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 13/100\n",
      "5948/5948 [==============================] - 0s 65us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 14/100\n",
      "5948/5948 [==============================] - 0s 60us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 15/100\n",
      "5948/5948 [==============================] - 0s 69us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 16/100\n",
      "5948/5948 [==============================] - 0s 61us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 17/100\n",
      "3488/5948 [================>.............] - ETA: 0s - loss: 0.7578 - acc: 0.5605"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(76, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(60, activation=tf.nn.softmax),\n",
    "    keras.layers.Dense(40, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(20, activation=tf.nn.softmax),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    keras.layers.Dense(9, activation=tf.nn.softmax) # for the 8 bins \n",
    "])\n",
    "\n",
    "for i in ['sparse_categorical_crossentropy', 'hinge_loss', 'log_loss','losses.mean_pairwise_squared_error']:\n",
    "    for j in [tf.train.AdamOptimizer(), tf.train.GradientDescentOptimizer(learning_rate=0.1),\n",
    "             tf.train.AdagradOptimizer(learning_rate = 0.1)]:\n",
    "        model.compile(optimizer=j, \n",
    "              loss=i,\n",
    "              metrics=['accuracy'])\n",
    "        model.fit(np.asarray(x_tra), y_tra, epochs=100)\n",
    "        test_loss, test_acc = model.evaluate(np.asarray(x_dev), y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(76, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(60, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(40, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(20, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    keras.layers.Dense(9, activation=tf.nn.softmax) # for the 8 bins \n",
    "])\n",
    "\n",
    "for i in ['sparse_categorical_crossentropy', 'hinge_loss', 'log_loss','losses.mean_pairwise_squared_error']:\n",
    "    for j in [tf.train.AdamOptimizer(), tf.train.GradientDescentOptimizer(learning_rate=0.1),\n",
    "             tf.train.AdagradOptimizer(learning_rate = 0.1)]:\n",
    "        model.compile(optimizer=j, \n",
    "              loss=i,\n",
    "              metrics=['accuracy'])\n",
    "        model.fit(np.asarray(x_tra), y_tra, epochs=50)\n",
    "        test_loss, test_acc = model.evaluate(np.asarray(x_dev), y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
