{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#parse data \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#label encoding on categorical data \n",
    "\n",
    "#FAMA 49CRSP Common Stocks \n",
    "df = pd.read_csv('ee6d2f60cdafb550.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 25 25 ... 22 10 47]\n",
      "       public_date  FFI49_desc  NFIRM  indret_ew  indret_vw  dpr_Median  \\\n",
      "12337     19930831          25      6   0.052904   0.068678       0.460   \n",
      "12383     19930930          25      6  -0.035253  -0.046242       0.460   \n",
      "12430     19931031          25      6   0.009319  -0.018318       0.460   \n",
      "12477     19931130          25      5  -0.022123  -0.014000       0.479   \n",
      "12524     19931231          25      5   0.096039   0.072342       0.479   \n",
      "12571     19940131          25      5   0.043779   0.009888       0.479   \n",
      "12618     19940228          25      6   0.012589   0.039537       0.394   \n",
      "12665     19940331          25      6  -0.012399  -0.052069       0.394   \n",
      "12712     19940430          25      6  -0.026779  -0.024747       0.394   \n",
      "12759     19940531          25      6  -0.001201   0.023906       0.394   \n",
      "12806     19940630          25      6  -0.001294  -0.037881       0.394   \n",
      "12853     19940731          25      6   0.023423   0.043118       0.394   \n",
      "12900     19940831          25      6   0.066949   0.055253       0.398   \n",
      "12947     19940930          25      6   0.004471  -0.026728       0.398   \n",
      "12994     19941031          25      6  -0.044276  -0.015566       0.398   \n",
      "13041     19941130          25      6  -0.040811  -0.035578       0.390   \n",
      "13088     19941231          25      6   0.007111   0.043045       0.390   \n",
      "13135     19950131          25      6   0.021079   0.023549       0.390   \n",
      "13182     19950228          25      6   0.058941   0.058359       0.422   \n",
      "13229     19950331          25      6   0.057380   0.023958       0.422   \n",
      "13276     19950430          25      6   0.062681   0.029973       0.422   \n",
      "13323     19950531          25      6   0.037249   0.031139       0.378   \n",
      "13370     19950630          25      6   0.056587   0.052070       0.378   \n",
      "13417     19950731          25      6   0.004115  -0.005009       0.378   \n",
      "13464     19950831          25      6  -0.000317   0.011758       0.422   \n",
      "13511     19950930          25      6   0.053875   0.015358       0.422   \n",
      "13558     19951031          25      6   0.004551  -0.003330       0.422   \n",
      "13605     19951130          25      6   0.047691   0.086727       0.409   \n",
      "13652     19951231          25      6   0.026102   0.035340       0.409   \n",
      "13699     19960131          25      6   0.044772   0.033413       0.409   \n",
      "...            ...         ...    ...        ...        ...         ...   \n",
      "25133     20161231          38      3   0.053255   0.054318       0.851   \n",
      "25134     20161231          39      3   0.023255   0.024929       0.437   \n",
      "25135     20161231          40     28  -0.009651   0.000257       0.000   \n",
      "25136     20161231          41      1  -0.036863  -0.036863       0.840   \n",
      "25137     20161231          42     15   0.020397   0.053427       0.291   \n",
      "25138     20161231          43      3  -0.066658  -0.075610       0.454   \n",
      "25139     20161231          44     15   0.007842   0.007611       0.294   \n",
      "25124     20161231          27      6  -0.004840  -0.002528       0.551   \n",
      "25132     20161231          35     34  -0.025950  -0.006974       0.297   \n",
      "25123     20161231          26     12  -0.002451  -0.008172       0.401   \n",
      "25110     20161231          12      6  -0.015227  -0.017383       0.087   \n",
      "25120     20161231          23     10   0.015469   0.015504       0.443   \n",
      "25141     20161231          46     33   0.036605   0.036090       0.641   \n",
      "25098     20161231           0      6   0.013718   0.016368       0.273   \n",
      "25100     20161231           2      5   0.037148   0.024898       0.232   \n",
      "25101     20161231           3     28   0.044543   0.044528       0.377   \n",
      "25102     20161231           4      4   0.010696   0.033009       0.425   \n",
      "25103     20161231           5      5  -0.004710  -0.006343       0.274   \n",
      "25106     20161231           8     17   0.001487   0.000991       0.316   \n",
      "25122     20161231          25     12   0.005462   0.005478       0.162   \n",
      "25107     20161231           9     12   0.008697   0.006098       0.484   \n",
      "25109     20161231          11      7  -0.050234  -0.012120       0.274   \n",
      "25111     20161231          14     16   0.003128   0.009955       0.375   \n",
      "25113     20161231          16     13   0.023076   0.036328       0.370   \n",
      "25114     20161231          17     14   0.046219   0.053876       0.616   \n",
      "25117     20161231          20      1  -0.057719  -0.057719       0.560   \n",
      "25118     20161231          21      6   0.018384   0.012505       0.510   \n",
      "25119     20161231          22      6  -0.012955   0.001926       0.026   \n",
      "25108     20161231          10     20   0.014731   0.029014       0.417   \n",
      "25142     20161231          47      9   0.006330   0.004922       0.392   \n",
      "\n",
      "       PEG_trailing_Median  bm_Median  CAPEI_Median  divyield_Median  \\\n",
      "12337                4.556      0.438        21.835           0.0229   \n",
      "12383                4.380      0.438        20.573           0.0245   \n",
      "12430                4.268      0.438        20.959           0.0251   \n",
      "12477                3.865      0.401        19.861           0.0273   \n",
      "12524                4.122      0.401        21.179           0.0255   \n",
      "12571                4.190      0.401        21.483           0.0212   \n",
      "12618               11.991      0.406        22.148           0.0234   \n",
      "12665               11.971      0.406        20.340           0.0233   \n",
      "12712               11.623      0.406        19.663           0.0251   \n",
      "12759                3.700      0.393        19.092           0.0245   \n",
      "12806                3.833      0.393        18.900           0.0252   \n",
      "12853                3.825      0.393        19.442           0.0248   \n",
      "12900                1.836      0.397        19.539           0.0239   \n",
      "12947                1.765      0.397        18.768           0.0239   \n",
      "12994                1.664      0.397        18.101           0.0244   \n",
      "13041                1.371      0.382        17.360           0.0269   \n",
      "13088                1.447      0.382        18.339           0.0271   \n",
      "13135                1.376      0.382        18.007           0.0260   \n",
      "13182                1.572      0.390        19.144           0.0249   \n",
      "13229                1.628      0.390        19.835           0.0246   \n",
      "13276                1.653      0.395        25.704           0.0237   \n",
      "13323                1.430      0.359        26.911           0.0223   \n",
      "13370                1.520      0.359        27.939           0.0212   \n",
      "13417                1.564      0.359        28.178           0.0217   \n",
      "13464                1.358      0.334        27.136           0.0214   \n",
      "13511                1.124      0.334        27.864           0.0212   \n",
      "13558                1.254      0.334        27.607           0.0216   \n",
      "13605                1.052      0.348        28.600           0.0204   \n",
      "13652                1.076      0.348        30.916           0.0197   \n",
      "13699                1.046      0.339        31.812           0.0174   \n",
      "...                    ...        ...           ...              ...   \n",
      "25133                5.987      0.273        28.409           0.0361   \n",
      "25134                0.997      0.166        24.194           0.0286   \n",
      "25135                1.724      0.177        27.234           0.0141   \n",
      "25136                3.066      0.523        33.958           0.0254   \n",
      "25137                0.765      0.502        22.358           0.0197   \n",
      "25138                0.611      0.233        25.309           0.0262   \n",
      "25139                1.132      0.432        21.286           0.0153   \n",
      "25124                1.497      0.101        31.726           0.0308   \n",
      "25132                1.589      0.305        21.762           0.0206   \n",
      "25123                0.807      0.312        16.528           0.0202   \n",
      "25110                0.500      0.655        14.286           0.0153   \n",
      "25120                3.701      0.212        26.668           0.0239   \n",
      "25141                2.104      0.798        23.026           0.0335   \n",
      "25098                1.627      0.241        22.032           0.0176   \n",
      "25100                0.446      0.542         9.784           0.0244   \n",
      "25101                2.449      0.877        15.996           0.0182   \n",
      "25102                8.059      0.185        26.059           0.0166   \n",
      "25103                0.568      0.293        41.512           0.0166   \n",
      "25106                1.826      0.219        32.407           0.0158   \n",
      "25122                2.457      0.265        23.877           0.0066   \n",
      "25107                1.148      0.295        24.150           0.0228   \n",
      "25109                0.845      0.210        19.595           0.0182   \n",
      "25111                1.298      0.178        24.618           0.0274   \n",
      "25113                1.429      0.596        22.645           0.0202   \n",
      "25114                1.332      0.219        25.145           0.0234   \n",
      "25117                2.528      0.033        23.666           0.0291   \n",
      "25118                2.234      0.411        18.186           0.0319   \n",
      "25119                1.222      0.396        21.419           0.0106   \n",
      "25108                1.664      0.262        24.592           0.0206   \n",
      "25142                3.060      0.244        22.458           0.0224   \n",
      "\n",
      "               ...            rect_turn_Median  sale_equity_Median  \\\n",
      "12337          ...                       5.494               3.146   \n",
      "12383          ...                       5.494               3.146   \n",
      "12430          ...                       5.685               3.027   \n",
      "12477          ...                       5.364               2.755   \n",
      "12524          ...                       5.364               2.755   \n",
      "12571          ...                       5.319               2.795   \n",
      "12618          ...                       4.789               2.451   \n",
      "12665          ...                       4.789               2.451   \n",
      "12712          ...                       4.789               2.525   \n",
      "12759          ...                       4.704               2.516   \n",
      "12806          ...                       4.704               2.516   \n",
      "12853          ...                       4.704               2.448   \n",
      "12900          ...                       4.589               2.449   \n",
      "12947          ...                       4.589               2.449   \n",
      "12994          ...                       4.589               2.513   \n",
      "13041          ...                       4.454               2.458   \n",
      "13088          ...                       4.454               2.458   \n",
      "13135          ...                       4.454               2.435   \n",
      "13182          ...                       4.443               2.568   \n",
      "13229          ...                       4.443               2.568   \n",
      "13276          ...                       4.443               2.526   \n",
      "13323          ...                       4.521               2.463   \n",
      "13370          ...                       4.521               2.463   \n",
      "13417          ...                       4.521               2.282   \n",
      "13464          ...                       4.551               2.364   \n",
      "13511          ...                       4.551               2.364   \n",
      "13558          ...                       4.551               2.510   \n",
      "13605          ...                       4.621               2.689   \n",
      "13652          ...                       4.621               2.689   \n",
      "13699          ...                       4.621               2.692   \n",
      "...            ...                         ...                 ...   \n",
      "25133          ...                      84.444               3.603   \n",
      "25134          ...                       9.870               1.646   \n",
      "25135          ...                       6.467               1.324   \n",
      "25136          ...                       9.640               2.084   \n",
      "25137          ...                       6.513               1.331   \n",
      "25138          ...                       5.333               2.850   \n",
      "25139          ...                      11.577               3.411   \n",
      "25124          ...                      24.406               3.634   \n",
      "25132          ...                      57.162               4.892   \n",
      "25123          ...                       4.811               1.908   \n",
      "25110          ...                      15.815               2.066   \n",
      "25120          ...                       7.705               3.133   \n",
      "25141          ...                       9.204               0.835   \n",
      "25098          ...                       6.365               2.672   \n",
      "25100          ...                       5.036               3.614   \n",
      "25101          ...                       0.092               0.422   \n",
      "25102          ...                       6.947               1.606   \n",
      "25103          ...                       7.212               1.779   \n",
      "25106          ...                       4.239               2.062   \n",
      "25122          ...                       6.038               1.086   \n",
      "25107          ...                       5.477               1.787   \n",
      "25109          ...                       9.522               2.378   \n",
      "25111          ...                       5.736               1.169   \n",
      "25113          ...                       4.244               0.499   \n",
      "25114          ...                      11.066               2.545   \n",
      "25117          ...                       5.158              15.531   \n",
      "25118          ...                       5.850               1.397   \n",
      "25119          ...                       7.180               1.817   \n",
      "25108          ...                       7.178               0.982   \n",
      "25142          ...                       9.550               4.712   \n",
      "\n",
      "       sale_invcap_Median  sale_nwc_Median  accrual_Median  rd_sale_Median  \\\n",
      "12337               2.601            9.285           0.049           0.068   \n",
      "12383               2.601            9.285           0.056           0.042   \n",
      "12430               2.484            9.285           0.056           0.042   \n",
      "12477               2.345            8.303           0.057           0.080   \n",
      "12524               2.345            8.303           0.057           0.080   \n",
      "12571               2.347            8.303           0.057           0.080   \n",
      "12618               2.121            7.341           0.038           0.068   \n",
      "12665               2.121            7.341           0.038           0.068   \n",
      "12712               2.163            7.321           0.038           0.068   \n",
      "12759               2.165            7.181           0.029           0.066   \n",
      "12806               2.165            7.181           0.029           0.066   \n",
      "12853               2.098            6.644           0.018           0.066   \n",
      "12900               2.110            6.244           0.001           0.064   \n",
      "12947               2.110            6.244           0.001           0.064   \n",
      "12994               2.162            6.483          -0.012           0.064   \n",
      "13041               2.132            6.226           0.012           0.063   \n",
      "13088               2.132            6.226           0.012           0.063   \n",
      "13135               2.121            6.171           0.012           0.063   \n",
      "13182               2.167            5.887           0.018           0.061   \n",
      "13229               2.167            5.887           0.018           0.061   \n",
      "13276               2.141            5.817           0.018           0.061   \n",
      "13323               2.157            5.438           0.016           0.059   \n",
      "13370               2.157            5.438           0.016           0.059   \n",
      "13417               2.032            5.562           0.016           0.059   \n",
      "13464               2.043            5.071           0.012           0.058   \n",
      "13511               2.043            5.071           0.012           0.058   \n",
      "13558               2.141            5.032           0.012           0.058   \n",
      "13605               2.123            5.813           0.007           0.056   \n",
      "13652               2.123            5.813           0.007           0.056   \n",
      "13699               2.116            5.819           0.007           0.056   \n",
      "...                   ...              ...             ...             ...   \n",
      "25133               1.202           37.073           0.000           0.010   \n",
      "25134               0.771            5.962           0.010           0.000   \n",
      "25135               0.848            2.536           0.092           0.142   \n",
      "25136               1.283            3.447           0.067           0.000   \n",
      "25137               0.642            5.943           0.029           0.000   \n",
      "25138               1.539            4.376           0.042           0.053   \n",
      "25139               1.975           22.973           0.054           0.000   \n",
      "25124               2.345           16.491           0.096           0.000   \n",
      "25132               2.979           11.434           0.064           0.000   \n",
      "25123               1.083            3.513           0.044           0.027   \n",
      "25110               1.686            9.334           0.022           0.000   \n",
      "25120               1.532           22.271           0.045           0.018   \n",
      "25141               0.412           28.583           0.048           0.000   \n",
      "25098               1.552            6.191           0.004           0.042   \n",
      "25100               1.527           10.102           0.052           0.038   \n",
      "25101               0.242            2.369           0.005           0.000   \n",
      "25102               0.768            4.789           0.023           0.000   \n",
      "25103               1.288            5.502           0.033           0.010   \n",
      "25106               0.837            8.357           0.043           0.000   \n",
      "25122               0.717            5.891           0.027           0.056   \n",
      "25107               0.819            5.277           0.045           0.028   \n",
      "25109               1.681            4.177           0.023           0.000   \n",
      "25111               0.771            2.366           0.035           0.174   \n",
      "25113               0.336            7.101           0.007           0.000   \n",
      "25114               1.555           12.893           0.039           0.007   \n",
      "25117               2.612           29.236           0.046           0.018   \n",
      "25118               0.955            2.551           0.032           0.130   \n",
      "25119               0.932           13.276           0.053           0.000   \n",
      "25108               0.838            2.092           0.052           0.099   \n",
      "25142               4.343            9.503           0.040           0.000   \n",
      "\n",
      "       adv_sale_Median  staff_sale_Median  PEG_1yrforward_Median  \\\n",
      "12337            0.013              0.000                 -0.333   \n",
      "12383            0.013              0.000                 -0.320   \n",
      "12430            0.013              0.000                 -0.350   \n",
      "12477            0.016              0.000                  0.677   \n",
      "12524            0.016              0.000                  0.722   \n",
      "12571            0.016              0.000                  0.814   \n",
      "12618            0.011              0.000                  1.108   \n",
      "12665            0.011              0.000                  1.013   \n",
      "12712            0.011              0.000                  0.897   \n",
      "12759            0.011              0.000                  0.816   \n",
      "12806            0.011              0.000                  0.866   \n",
      "12853            0.011              0.000                  1.522   \n",
      "12900            0.012              0.000                  1.517   \n",
      "12947            0.012              0.000                  1.456   \n",
      "12994            0.012              0.000                  0.806   \n",
      "13041            0.008              0.000                  0.923   \n",
      "13088            0.008              0.000                  0.975   \n",
      "13135            0.008              0.000                  1.063   \n",
      "13182            0.000              0.000                  1.046   \n",
      "13229            0.000              0.000                  1.123   \n",
      "13276            0.000              0.000                  1.136   \n",
      "13323            0.000              0.000                  1.055   \n",
      "13370            0.000              0.000                  1.134   \n",
      "13417            0.000              0.000                  1.098   \n",
      "13464            0.000              0.000                  1.022   \n",
      "13511            0.000              0.000                  1.002   \n",
      "13558            0.000              0.000                  0.935   \n",
      "13605            0.000              0.000                  0.873   \n",
      "13652            0.000              0.000                  0.863   \n",
      "13699            0.000              0.000                  0.840   \n",
      "...                ...                ...                    ...   \n",
      "25133            0.013              0.000                  3.095   \n",
      "25134            0.078              0.000                  0.567   \n",
      "25135            0.017              0.000                  1.762   \n",
      "25136            0.000              0.000                  1.132   \n",
      "25137            0.040              0.000                  0.458   \n",
      "25138            0.092              0.000                  8.788   \n",
      "25139            0.000              0.245                  2.168   \n",
      "25124            0.022              0.087                  1.605   \n",
      "25132            0.011              0.000                  1.195   \n",
      "25123            0.000              0.000                 -0.713   \n",
      "25110            0.002              0.000                  0.931   \n",
      "25120            0.056              0.000                  3.005   \n",
      "25141            0.000              0.000                  2.240   \n",
      "25098            0.000              0.000                  2.150   \n",
      "25100            0.023              0.000                  0.164   \n",
      "25101            0.015              0.306                  1.301   \n",
      "25102            0.092              0.000                  2.981   \n",
      "25103            0.000              0.000                  0.718   \n",
      "25106            0.000              0.000                  2.369   \n",
      "25122            0.000              0.000                  2.091   \n",
      "25107            0.000              0.000                  1.718   \n",
      "25109            0.047              0.000                  2.371   \n",
      "25111            0.014              0.000                  1.328   \n",
      "25113            0.000              0.242                  1.433   \n",
      "25114            0.029              0.000                  2.894   \n",
      "25117            0.000              0.000                  2.765   \n",
      "25118            0.004              0.000                  1.552   \n",
      "25119            0.000              0.228                  1.508   \n",
      "25108            0.000              0.000                  1.713   \n",
      "25142            0.000              0.000                 -4.237   \n",
      "\n",
      "       PEG_ltgforward_Median  \n",
      "12337                  1.572  \n",
      "12383                  1.445  \n",
      "12430                  1.422  \n",
      "12477                  1.848  \n",
      "12524                  1.962  \n",
      "12571                  1.930  \n",
      "12618                  1.721  \n",
      "12665                  1.587  \n",
      "12712                  1.557  \n",
      "12759                  1.410  \n",
      "12806                  1.438  \n",
      "12853                  1.525  \n",
      "12900                  1.538  \n",
      "12947                  1.524  \n",
      "12994                  1.418  \n",
      "13041                  1.333  \n",
      "13088                  1.429  \n",
      "13135                  1.478  \n",
      "13182                  1.517  \n",
      "13229                  1.610  \n",
      "13276                  1.671  \n",
      "13323                  1.668  \n",
      "13370                  1.722  \n",
      "13417                  1.679  \n",
      "13464                  1.563  \n",
      "13511                  1.687  \n",
      "13558                  1.653  \n",
      "13605                  1.633  \n",
      "13652                  1.570  \n",
      "13699                  1.565  \n",
      "...                      ...  \n",
      "25133                  3.081  \n",
      "25134                  1.997  \n",
      "25135                  2.425  \n",
      "25136                  1.401  \n",
      "25137                  1.019  \n",
      "25138                  2.122  \n",
      "25139                  2.081  \n",
      "25124                  1.987  \n",
      "25132                  1.778  \n",
      "25123                  2.365  \n",
      "25110                  1.580  \n",
      "25120                  3.063  \n",
      "25141                  3.676  \n",
      "25098                  2.645  \n",
      "25100                 -1.294  \n",
      "25101                  1.945  \n",
      "25102                  3.205  \n",
      "25103                  1.579  \n",
      "25106                  2.179  \n",
      "25122                  2.545  \n",
      "25107                  2.572  \n",
      "25109                  2.236  \n",
      "25111                  2.379  \n",
      "25113                  1.546  \n",
      "25114                  3.445  \n",
      "25117                  2.721  \n",
      "25118                  1.875  \n",
      "25119                  1.884  \n",
      "25108                  2.218  \n",
      "25142                  2.072  \n",
      "\n",
      "[9297 rows x 76 columns]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing \n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#preprocessing here\n",
    "#sort by date \n",
    "df = df.sort_values(by = 'public_date', ascending = True)\n",
    "\n",
    "#encode integer categories into numbers \n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df.FFI49_desc)\n",
    "print(integer_encoded)\n",
    "df.FFI49_desc = integer_encoded\n",
    "df.divyield_Median = [float(x.strip('%'))/100 for x in df.divyield_Median]\n",
    "print(df)\n",
    "\n",
    "#todo: https://www.gsb.stanford.edu/library/articles/databases/links/financial-ratios-suite?fbclid=IwAR0EGNGk9DdxQjEHfdaoUhdY3tNzAWDogYDzuuJi1zT_muL-uJtWQw19Fzk\n",
    "\n",
    "#get output first \n",
    "ewlabels = df.indret_ew\n",
    "vwlabels = df.indret_vw\n",
    "\n",
    "#3year on year change as a prediction feature, raw pct change \n",
    "yoythree = ewlabels.diff(periods = 3)\n",
    "#3 years rolling percent change, averaged ie. (y1-y2 + (y3-y2)change)/2 \n",
    "rollavgpct = ewlabels.rolling(3).mean()\n",
    "\n",
    "#drop first 3 years\n",
    "df = df.iloc[3:]\n",
    "ewlabels = ewlabels.iloc[3:]\n",
    "yoythree = yoythree.iloc[3:]\n",
    "#yoypctthree = yoypctthree.iloc[3:]\n",
    "rollavgpct = rollavgpct.iloc[3:]\n",
    "\n",
    "#add -1 and 1 so the bins will take on bins to be equal and set to max -1 and 1\n",
    "extrema = pd.Series([-1,1])\n",
    "ewnlabels = ewlabels.append(extrema)\n",
    "\n",
    "#make a new output (bucket by percentage?)\n",
    "enc = KBinsDiscretizer(n_bins=8, encode='ordinal',strategy = 'uniform')\n",
    "ewnlabels = np.asarray(ewnlabels)\n",
    "ewnlabels = ewnlabels.reshape((-1,1))\n",
    "labels_binned = enc.fit_transform(ewnlabels)\n",
    "\n",
    "labels_binned = labels_binned[:-2]\n",
    "\n",
    "#1 Split-Timer series data, 0.64 Train, 0.16 dev, 0.2 Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, labels_binned, test_size = 0.2, shuffle = False)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(df, ewnlabels, test_size = 0.2, shuffle = False)\n",
    "x_tra, x_dev, y_tra, y_dev = train_test_split(x_train, y_train, test_size = 0.2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       public_date  FFI49_desc  NFIRM  indret_ew  indret_vw  dpr_Median  \\\n",
      "12477     19931130          25      5  -0.022123  -0.014000       0.479   \n",
      "12524     19931231          25      5   0.096039   0.072342       0.479   \n",
      "12571     19940131          25      5   0.043779   0.009888       0.479   \n",
      "12618     19940228          25      6   0.012589   0.039537       0.394   \n",
      "12665     19940331          25      6  -0.012399  -0.052069       0.394   \n",
      "12712     19940430          25      6  -0.026779  -0.024747       0.394   \n",
      "12759     19940531          25      6  -0.001201   0.023906       0.394   \n",
      "12806     19940630          25      6  -0.001294  -0.037881       0.394   \n",
      "12853     19940731          25      6   0.023423   0.043118       0.394   \n",
      "12900     19940831          25      6   0.066949   0.055253       0.398   \n",
      "12947     19940930          25      6   0.004471  -0.026728       0.398   \n",
      "12994     19941031          25      6  -0.044276  -0.015566       0.398   \n",
      "13041     19941130          25      6  -0.040811  -0.035578       0.390   \n",
      "13088     19941231          25      6   0.007111   0.043045       0.390   \n",
      "13135     19950131          25      6   0.021079   0.023549       0.390   \n",
      "13182     19950228          25      6   0.058941   0.058359       0.422   \n",
      "13229     19950331          25      6   0.057380   0.023958       0.422   \n",
      "13276     19950430          25      6   0.062681   0.029973       0.422   \n",
      "13323     19950531          25      6   0.037249   0.031139       0.378   \n",
      "13370     19950630          25      6   0.056587   0.052070       0.378   \n",
      "13417     19950731          25      6   0.004115  -0.005009       0.378   \n",
      "13464     19950831          25      6  -0.000317   0.011758       0.422   \n",
      "13511     19950930          25      6   0.053875   0.015358       0.422   \n",
      "13558     19951031          25      6   0.004551  -0.003330       0.422   \n",
      "13605     19951130          25      6   0.047691   0.086727       0.409   \n",
      "13652     19951231          25      6   0.026102   0.035340       0.409   \n",
      "13699     19960131          25      6   0.044772   0.033413       0.409   \n",
      "13745     19960229          25      6   0.003002  -0.025817       0.382   \n",
      "13791     19960331          25      6  -0.020940   0.020593       0.382   \n",
      "13837     19960430          25      6   0.057884   0.024875       0.382   \n",
      "...            ...         ...    ...        ...        ...         ...   \n",
      "21410     20100331           4      5   0.078739   0.059782       0.423   \n",
      "21411     20100331           5      6   0.082778   0.090577       0.543   \n",
      "21412     20100331           6      4   0.068238   0.055803       0.105   \n",
      "21413     20100331           7      2   0.093421   0.092510       0.049   \n",
      "21414     20100331           8     13   0.076576   0.069945       0.239   \n",
      "21406     20100331           0      7   0.103798   0.108410       0.312   \n",
      "21416     20100331          10     30   0.074012   0.078976       0.243   \n",
      "21417     20100331          11      5   0.084022   0.083646       0.329   \n",
      "21418     20100331          12      6   0.061611   0.069313       0.000   \n",
      "21419     20100331          13      3   0.018897  -0.026772       0.152   \n",
      "21420     20100331          14     21   0.035427   0.019780       0.000   \n",
      "21421     20100331          15      3   0.072547   0.070492       0.872   \n",
      "21422     20100331          16     14   0.083292   0.075081       0.362   \n",
      "21423     20100331          17     15   0.042380   0.036261       0.437   \n",
      "21415     20100331           9     11   0.060225   0.072472       0.469   \n",
      "21483     20100430          31      4   0.051587   0.040814       0.286   \n",
      "21482     20100430          30     24   0.034347   0.037883       0.274   \n",
      "21481     20100430          29      3  -0.000077  -0.063170       0.156   \n",
      "21477     20100430          25      9   0.062083   0.060680       0.018   \n",
      "21479     20100430          27      6   0.098346   0.081673       0.336   \n",
      "21478     20100430          26     13   0.049836   0.058474       0.329   \n",
      "21484     20100430          32      8   0.044267   0.038288       0.497   \n",
      "21480     20100430          28     12   0.016479  -0.001779       0.108   \n",
      "21485     20100430          33      5   0.053862   0.012300       0.403   \n",
      "21492     20100430          41      6  -0.090854  -0.062553       1.114   \n",
      "21489     20100430          38      4   0.001207  -0.024248       0.772   \n",
      "21491     20100430          40     28   0.013151   0.001585       0.000   \n",
      "21493     20100430          42     19   0.047229   0.028011       0.333   \n",
      "21494     20100430          43      3  -0.045039  -0.021469       0.408   \n",
      "21495     20100430          44      9   0.039780   0.042859       0.335   \n",
      "\n",
      "       PEG_trailing_Median  bm_Median  CAPEI_Median  divyield_Median  \\\n",
      "12477                3.865      0.401        19.861           0.0273   \n",
      "12524                4.122      0.401        21.179           0.0255   \n",
      "12571                4.190      0.401        21.483           0.0212   \n",
      "12618               11.991      0.406        22.148           0.0234   \n",
      "12665               11.971      0.406        20.340           0.0233   \n",
      "12712               11.623      0.406        19.663           0.0251   \n",
      "12759                3.700      0.393        19.092           0.0245   \n",
      "12806                3.833      0.393        18.900           0.0252   \n",
      "12853                3.825      0.393        19.442           0.0248   \n",
      "12900                1.836      0.397        19.539           0.0239   \n",
      "12947                1.765      0.397        18.768           0.0239   \n",
      "12994                1.664      0.397        18.101           0.0244   \n",
      "13041                1.371      0.382        17.360           0.0269   \n",
      "13088                1.447      0.382        18.339           0.0271   \n",
      "13135                1.376      0.382        18.007           0.0260   \n",
      "13182                1.572      0.390        19.144           0.0249   \n",
      "13229                1.628      0.390        19.835           0.0246   \n",
      "13276                1.653      0.395        25.704           0.0237   \n",
      "13323                1.430      0.359        26.911           0.0223   \n",
      "13370                1.520      0.359        27.939           0.0212   \n",
      "13417                1.564      0.359        28.178           0.0217   \n",
      "13464                1.358      0.334        27.136           0.0214   \n",
      "13511                1.124      0.334        27.864           0.0212   \n",
      "13558                1.254      0.334        27.607           0.0216   \n",
      "13605                1.052      0.348        28.600           0.0204   \n",
      "13652                1.076      0.348        30.916           0.0197   \n",
      "13699                1.046      0.339        31.812           0.0174   \n",
      "13745                0.043      0.305        28.287           0.0172   \n",
      "13791                0.050      0.305        26.299           0.0186   \n",
      "13837                0.051      0.305        28.086           0.0175   \n",
      "...                    ...        ...           ...              ...   \n",
      "21410                2.764      0.264        16.811           0.0256   \n",
      "21411                0.756      0.590        15.854           0.0175   \n",
      "21412               47.898      0.397         2.674           0.0264   \n",
      "21413                1.252      0.325       243.770           0.0075   \n",
      "21414                0.878      0.475        20.651           0.0179   \n",
      "21406                2.382      0.313        17.894           0.0153   \n",
      "21416                0.525      0.350        17.863           0.0191   \n",
      "21417                2.972      0.385        16.232           0.0147   \n",
      "21418                0.220      0.748         4.729           0.0108   \n",
      "21419                0.798      0.334        21.381           0.0061   \n",
      "21420                0.592      0.344        21.225           0.0304   \n",
      "21421               27.928      0.290        16.364           0.0236   \n",
      "21422                4.623      0.816        18.291           0.0128   \n",
      "21423                1.140      0.379        19.072           0.0279   \n",
      "21415                1.992      0.314        17.306           0.0213   \n",
      "21483                0.386      0.561        27.768           0.0304   \n",
      "21482                0.536      0.753        12.894           0.0101   \n",
      "21481                6.223      0.433        27.429           0.0079   \n",
      "21477                1.569      0.377        24.980           0.0087   \n",
      "21479                0.930      0.211        22.424           0.0176   \n",
      "21478                0.924      0.395        17.430           0.0169   \n",
      "21484                1.116      0.558        20.549           0.0237   \n",
      "21480                1.013      0.325        21.219           0.0104   \n",
      "21485                0.433      0.272        18.083           0.0196   \n",
      "21492               -0.064      0.509        12.455           0.0119   \n",
      "21489                1.522      0.196        14.002           0.0586   \n",
      "21491                0.978      0.279        30.421           0.0124   \n",
      "21493                0.975      0.704        15.556           0.0284   \n",
      "21494                3.733      0.364        18.511           0.0293   \n",
      "21495                2.373      0.594        21.950           0.0143   \n",
      "\n",
      "               ...            rect_turn_Median  sale_equity_Median  \\\n",
      "12477          ...                       5.364               2.755   \n",
      "12524          ...                       5.364               2.755   \n",
      "12571          ...                       5.319               2.795   \n",
      "12618          ...                       4.789               2.451   \n",
      "12665          ...                       4.789               2.451   \n",
      "12712          ...                       4.789               2.525   \n",
      "12759          ...                       4.704               2.516   \n",
      "12806          ...                       4.704               2.516   \n",
      "12853          ...                       4.704               2.448   \n",
      "12900          ...                       4.589               2.449   \n",
      "12947          ...                       4.589               2.449   \n",
      "12994          ...                       4.589               2.513   \n",
      "13041          ...                       4.454               2.458   \n",
      "13088          ...                       4.454               2.458   \n",
      "13135          ...                       4.454               2.435   \n",
      "13182          ...                       4.443               2.568   \n",
      "13229          ...                       4.443               2.568   \n",
      "13276          ...                       4.443               2.526   \n",
      "13323          ...                       4.521               2.463   \n",
      "13370          ...                       4.521               2.463   \n",
      "13417          ...                       4.521               2.282   \n",
      "13464          ...                       4.551               2.364   \n",
      "13511          ...                       4.551               2.364   \n",
      "13558          ...                       4.551               2.510   \n",
      "13605          ...                       4.621               2.689   \n",
      "13652          ...                       4.621               2.689   \n",
      "13699          ...                       4.621               2.692   \n",
      "13745          ...                       4.713               2.683   \n",
      "13791          ...                       4.713               2.683   \n",
      "13837          ...                       4.713               2.673   \n",
      "...            ...                         ...                 ...   \n",
      "21410          ...                       5.642               1.325   \n",
      "21411          ...                       6.260               1.879   \n",
      "21412          ...                       6.528               3.361   \n",
      "21413          ...                       9.197               4.620   \n",
      "21414          ...                       5.295               1.489   \n",
      "21406          ...                       6.010               2.946   \n",
      "21416          ...                       7.904               1.154   \n",
      "21417          ...                       9.419               1.893   \n",
      "21418          ...                       6.727               1.417   \n",
      "21419          ...                      14.468               2.142   \n",
      "21420          ...                       6.007               0.924   \n",
      "21421          ...                       5.644               2.281   \n",
      "21422          ...                       6.817               0.686   \n",
      "21423          ...                      11.937               4.262   \n",
      "21415          ...                       5.592               2.626   \n",
      "21483          ...                       7.139               1.634   \n",
      "21482          ...                       6.127               0.644   \n",
      "21481          ...                       9.977               0.923   \n",
      "21477          ...                       5.869               1.101   \n",
      "21479          ...                      28.244               3.780   \n",
      "21478          ...                       4.698               1.575   \n",
      "21484          ...                       6.947               2.683   \n",
      "21480          ...                       5.751               1.152   \n",
      "21485          ...                      10.043               3.425   \n",
      "21492          ...                       7.175               1.516   \n",
      "21489          ...                      49.648               4.257   \n",
      "21491          ...                       5.808               1.110   \n",
      "21493          ...                       8.898               1.378   \n",
      "21494          ...                       6.412               2.551   \n",
      "21495          ...                       9.141               1.894   \n",
      "\n",
      "       sale_invcap_Median  sale_nwc_Median  accrual_Median  rd_sale_Median  \\\n",
      "12477               2.345            8.303           0.057           0.080   \n",
      "12524               2.345            8.303           0.057           0.080   \n",
      "12571               2.347            8.303           0.057           0.080   \n",
      "12618               2.121            7.341           0.038           0.068   \n",
      "12665               2.121            7.341           0.038           0.068   \n",
      "12712               2.163            7.321           0.038           0.068   \n",
      "12759               2.165            7.181           0.029           0.066   \n",
      "12806               2.165            7.181           0.029           0.066   \n",
      "12853               2.098            6.644           0.018           0.066   \n",
      "12900               2.110            6.244           0.001           0.064   \n",
      "12947               2.110            6.244           0.001           0.064   \n",
      "12994               2.162            6.483          -0.012           0.064   \n",
      "13041               2.132            6.226           0.012           0.063   \n",
      "13088               2.132            6.226           0.012           0.063   \n",
      "13135               2.121            6.171           0.012           0.063   \n",
      "13182               2.167            5.887           0.018           0.061   \n",
      "13229               2.167            5.887           0.018           0.061   \n",
      "13276               2.141            5.817           0.018           0.061   \n",
      "13323               2.157            5.438           0.016           0.059   \n",
      "13370               2.157            5.438           0.016           0.059   \n",
      "13417               2.032            5.562           0.016           0.059   \n",
      "13464               2.043            5.071           0.012           0.058   \n",
      "13511               2.043            5.071           0.012           0.058   \n",
      "13558               2.141            5.032           0.012           0.058   \n",
      "13605               2.123            5.813           0.007           0.056   \n",
      "13652               2.123            5.813           0.007           0.056   \n",
      "13699               2.116            5.819           0.007           0.056   \n",
      "13745               2.135            6.049           0.022           0.055   \n",
      "13791               2.135            6.049           0.022           0.055   \n",
      "13837               2.118            6.007           0.022           0.055   \n",
      "...                   ...              ...             ...             ...   \n",
      "21410               1.019            8.091           0.031           0.000   \n",
      "21411               1.145            4.666           0.067           0.010   \n",
      "21412               1.592           37.687           0.086           0.000   \n",
      "21413               1.657           12.058           0.052           0.006   \n",
      "21414               1.489            7.596           0.057           0.000   \n",
      "21406               1.782            7.494           0.030           0.038   \n",
      "21416               0.917            2.232           0.081           0.139   \n",
      "21417               1.519            3.373           0.093           0.000   \n",
      "21418               0.868            7.559           0.085           0.000   \n",
      "21419               1.055            4.977           0.054           0.000   \n",
      "21420               0.711            2.479           0.048           0.157   \n",
      "21421               1.515            6.470           0.087           0.033   \n",
      "21422               0.472            2.022           0.022           0.000   \n",
      "21423               1.876           11.184           0.056           0.005   \n",
      "21415               1.491            5.110           0.070           0.027   \n",
      "21483               0.624           45.613           0.049           0.000   \n",
      "21482               0.508           11.527           0.087           0.000   \n",
      "21481               0.738            3.683           0.050           0.000   \n",
      "21477               0.770            3.428           0.054           0.059   \n",
      "21479               2.355           28.506           0.088           0.000   \n",
      "21478               1.040            3.141           0.069           0.031   \n",
      "21484               1.404            6.602           0.080           0.013   \n",
      "21480               0.947            2.815           0.051           0.061   \n",
      "21485               1.741           13.704           0.105           0.000   \n",
      "21492               1.013            3.589           0.073           0.004   \n",
      "21489               1.182            7.145           0.032           0.009   \n",
      "21491               0.921            2.581           0.077           0.121   \n",
      "21493               0.608            6.353           0.082           0.000   \n",
      "21494               1.681            3.636           0.088           0.045   \n",
      "21495               1.177           13.908           0.041           0.000   \n",
      "\n",
      "       adv_sale_Median  staff_sale_Median  PEG_1yrforward_Median  \\\n",
      "12477            0.016              0.000                  0.677   \n",
      "12524            0.016              0.000                  0.722   \n",
      "12571            0.016              0.000                  0.814   \n",
      "12618            0.011              0.000                  1.108   \n",
      "12665            0.011              0.000                  1.013   \n",
      "12712            0.011              0.000                  0.897   \n",
      "12759            0.011              0.000                  0.816   \n",
      "12806            0.011              0.000                  0.866   \n",
      "12853            0.011              0.000                  1.522   \n",
      "12900            0.012              0.000                  1.517   \n",
      "12947            0.012              0.000                  1.456   \n",
      "12994            0.012              0.000                  0.806   \n",
      "13041            0.008              0.000                  0.923   \n",
      "13088            0.008              0.000                  0.975   \n",
      "13135            0.008              0.000                  1.063   \n",
      "13182            0.000              0.000                  1.046   \n",
      "13229            0.000              0.000                  1.123   \n",
      "13276            0.000              0.000                  1.136   \n",
      "13323            0.000              0.000                  1.055   \n",
      "13370            0.000              0.000                  1.134   \n",
      "13417            0.000              0.000                  1.098   \n",
      "13464            0.000              0.000                  1.022   \n",
      "13511            0.000              0.000                  1.002   \n",
      "13558            0.000              0.000                  0.935   \n",
      "13605            0.000              0.000                  0.873   \n",
      "13652            0.000              0.000                  0.863   \n",
      "13699            0.000              0.000                  0.840   \n",
      "13745            0.000              0.000                  0.906   \n",
      "13791            0.000              0.000                  1.060   \n",
      "13837            0.000              0.000                  1.169   \n",
      "...                ...                ...                    ...   \n",
      "21410            0.090              0.000                  1.135   \n",
      "21411            0.007              0.000                  0.602   \n",
      "21412            0.005              0.000                  1.236   \n",
      "21413            0.000              0.000                  2.336   \n",
      "21414            0.000              0.000                  1.709   \n",
      "21406            0.000              0.000                 -1.855   \n",
      "21416            0.000              0.000                  0.260   \n",
      "21417            0.034              0.000                 -1.109   \n",
      "21418            0.004              0.000                 -0.088   \n",
      "21419            0.000              0.000                  0.560   \n",
      "21420            0.000              0.000                  1.528   \n",
      "21421            0.000              0.000                  2.434   \n",
      "21422            0.000              0.232                  0.380   \n",
      "21423            0.022              0.000                  1.214   \n",
      "21415            0.000              0.000                  1.271   \n",
      "21483            0.000              0.000                  1.908   \n",
      "21482            0.000              0.000                 -0.123   \n",
      "21481            0.000              0.000                  0.360   \n",
      "21477            0.001              0.000                  1.735   \n",
      "21479            0.027              0.000                  1.519   \n",
      "21478            0.000              0.000                  0.214   \n",
      "21484            0.000              0.000                  1.433   \n",
      "21480            0.000              0.000                  2.045   \n",
      "21485            0.000              0.000                  0.470   \n",
      "21492            0.000              0.000                 -0.078   \n",
      "21489            0.012              0.000                  1.891   \n",
      "21491            0.012              0.000                  1.848   \n",
      "21493            0.024              0.000                  1.122   \n",
      "21494            0.101              0.000                  0.777   \n",
      "21495            0.000              0.291                  0.927   \n",
      "\n",
      "       PEG_ltgforward_Median  \n",
      "12477                  1.848  \n",
      "12524                  1.962  \n",
      "12571                  1.930  \n",
      "12618                  1.721  \n",
      "12665                  1.587  \n",
      "12712                  1.557  \n",
      "12759                  1.410  \n",
      "12806                  1.438  \n",
      "12853                  1.525  \n",
      "12900                  1.538  \n",
      "12947                  1.524  \n",
      "12994                  1.418  \n",
      "13041                  1.333  \n",
      "13088                  1.429  \n",
      "13135                  1.478  \n",
      "13182                  1.517  \n",
      "13229                  1.610  \n",
      "13276                  1.671  \n",
      "13323                  1.668  \n",
      "13370                  1.722  \n",
      "13417                  1.679  \n",
      "13464                  1.563  \n",
      "13511                  1.687  \n",
      "13558                  1.653  \n",
      "13605                  1.633  \n",
      "13652                  1.570  \n",
      "13699                  1.565  \n",
      "13745                  1.343  \n",
      "13791                  1.080  \n",
      "13837                  1.231  \n",
      "...                      ...  \n",
      "21410                  1.177  \n",
      "21411                  2.147  \n",
      "21412                  0.300  \n",
      "21413                  3.522  \n",
      "21414                  1.656  \n",
      "21406                  1.765  \n",
      "21416                  1.533  \n",
      "21417                  2.043  \n",
      "21418                  0.352  \n",
      "21419                  2.933  \n",
      "21420                  1.788  \n",
      "21421                  2.616  \n",
      "21422                  1.229  \n",
      "21423                  1.657  \n",
      "21415                  2.696  \n",
      "21483                  1.572  \n",
      "21482                  0.718  \n",
      "21481                  2.740  \n",
      "21477                  1.698  \n",
      "21479                  1.239  \n",
      "21478                  1.934  \n",
      "21484                  1.816  \n",
      "21480                  1.716  \n",
      "21485                  1.667  \n",
      "21492                 -0.525  \n",
      "21489                  1.994  \n",
      "21491                  1.860  \n",
      "21493                  2.125  \n",
      "21494                  1.547  \n",
      "21495                  2.214  \n",
      "\n",
      "[5948 rows x 76 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, input_dim=76, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\carol\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\carol\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5948/5948 [==============================] - 1s 120us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 2/25\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 3/25\n",
      "5948/5948 [==============================] - 0s 70us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 4/25\n",
      "5948/5948 [==============================] - 1s 124us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 5/25\n",
      "5948/5948 [==============================] - 1s 106us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 6/25\n",
      "5948/5948 [==============================] - 1s 122us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 7/25\n",
      "5948/5948 [==============================] - 1s 87us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 8/25\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 9/25\n",
      "5948/5948 [==============================] - 1s 93us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 10/25\n",
      "5948/5948 [==============================] - 1s 98us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 11/25\n",
      "5948/5948 [==============================] - 1s 94us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 12/25\n",
      "5948/5948 [==============================] - 1s 99us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 13/25\n",
      "5948/5948 [==============================] - 1s 91us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 14/25\n",
      "5948/5948 [==============================] - 1s 85us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 15/25\n",
      "5948/5948 [==============================] - 0s 52us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 16/25\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 17/25\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 18/25\n",
      "5948/5948 [==============================] - 0s 69us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 19/25\n",
      "5948/5948 [==============================] - 1s 94us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 20/25\n",
      "5948/5948 [==============================] - 1s 87us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 21/25\n",
      "5948/5948 [==============================] - 1s 104us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 22/25\n",
      "5948/5948 [==============================] - 1s 117us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 23/25\n",
      "5948/5948 [==============================] - 1s 88us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 24/25\n",
      "5948/5948 [==============================] - 0s 78us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "Epoch 25/25\n",
      "5948/5948 [==============================] - 1s 101us/step - loss: -41.0112 - acc: 0.0000e+00\n",
      "----------------------------------------------------------\n",
      "5948/5948 [==============================] - 1s 108us/step\n",
      "\n",
      "acc: 0.00%\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "--------------------------------------------\n",
      "7.115669132481506\n"
     ]
    }
   ],
   "source": [
    "#tutorial keras practice\n",
    "#https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "\n",
    "#####IGNORE THIS!!!!!!!!\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Softmax\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy\n",
    "\n",
    "model = Sequential()\n",
    "#parameters = number of neurons, initialization method, activation function\n",
    "model.add(Dense(32, input_dim=76, init = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(16, init = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_tra, y_tra, epochs=25, batch_size=32)\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "scores = model.evaluate(x_tra,y_tra)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "\n",
    "y_devpred = model.predict(x_dev)\n",
    "print(\"--------------------------------------------\")\n",
    "print(mean_squared_error(y_dev,y_devpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5948/5948 [==============================] - 1s 220us/step - loss: 5469.6471 - mean_squared_error: 5469.6471 - mean_absolute_error: 26.7449 - mean_absolute_percentage_error: 763.7899 - cosine_proximity: -0.5696\n",
      "Epoch 2/25\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.4356 - mean_squared_error: 0.4356 - mean_absolute_error: 0.5487 - mean_absolute_percentage_error: 15.9883 - cosine_proximity: -1.0000\n",
      "Epoch 3/25\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 1.0050 - mean_squared_error: 1.0050 - mean_absolute_error: 0.8236 - mean_absolute_percentage_error: 23.7582 - cosine_proximity: -1.0000\n",
      "Epoch 4/25\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 235.0561 - mean_squared_error: 235.0561 - mean_absolute_error: 8.0256 - mean_absolute_percentage_error: 229.8755 - cosine_proximity: -0.5696\n",
      "Epoch 5/25\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 236.3723 - mean_squared_error: 236.3723 - mean_absolute_error: 7.2190 - mean_absolute_percentage_error: 207.1605 - cosine_proximity: -0.6664\n",
      "Epoch 6/25\n",
      "5948/5948 [==============================] - 0s 69us/step - loss: 197.4075 - mean_squared_error: 197.4075 - mean_absolute_error: 6.8570 - mean_absolute_percentage_error: 196.2848 - cosine_proximity: -0.6769\n",
      "Epoch 7/25\n",
      "5948/5948 [==============================] - 0s 69us/step - loss: 1875.0047 - mean_squared_error: 1875.0047 - mean_absolute_error: 19.2707 - mean_absolute_percentage_error: 551.1158 - cosine_proximity: -0.4079\n",
      "Epoch 8/25\n",
      "5948/5948 [==============================] - 1s 96us/step - loss: 12.9290 - mean_squared_error: 12.9290 - mean_absolute_error: 3.5579 - mean_absolute_percentage_error: 99.5827 - cosine_proximity: -1.0000\n",
      "Epoch 9/25\n",
      "5948/5948 [==============================] - 0s 81us/step - loss: 12.7545 - mean_squared_error: 12.7545 - mean_absolute_error: 3.5332 - mean_absolute_percentage_error: 98.8764 - cosine_proximity: -1.0000\n",
      "Epoch 10/25\n",
      "5948/5948 [==============================] - 0s 80us/step - loss: 12.5616 - mean_squared_error: 12.5616 - mean_absolute_error: 3.5058 - mean_absolute_percentage_error: 98.0927 - cosine_proximity: -1.0000\n",
      "Epoch 11/25\n",
      "5948/5948 [==============================] - 0s 72us/step - loss: 12.3498 - mean_squared_error: 12.3498 - mean_absolute_error: 3.4755 - mean_absolute_percentage_error: 97.2233 - cosine_proximity: -1.0000\n",
      "Epoch 12/25\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 12.1189 - mean_squared_error: 12.1189 - mean_absolute_error: 3.4421 - mean_absolute_percentage_error: 96.2680 - cosine_proximity: -1.0000\n",
      "Epoch 13/25\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 11.8683 - mean_squared_error: 11.8683 - mean_absolute_error: 3.4056 - mean_absolute_percentage_error: 95.2220 - cosine_proximity: -1.0000\n",
      "Epoch 14/25\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 11.5979 - mean_squared_error: 11.5979 - mean_absolute_error: 3.3655 - mean_absolute_percentage_error: 94.0717 - cosine_proximity: -1.0000\n",
      "Epoch 15/25\n",
      "5948/5948 [==============================] - 0s 66us/step - loss: 11.3074 - mean_squared_error: 11.3074 - mean_absolute_error: 3.3221 - mean_absolute_percentage_error: 92.8327 - cosine_proximity: -1.0000\n",
      "Epoch 16/25\n",
      "5948/5948 [==============================] - 0s 84us/step - loss: 10.9969 - mean_squared_error: 10.9969 - mean_absolute_error: 3.2751 - mean_absolute_percentage_error: 91.4839 - cosine_proximity: -1.0000\n",
      "Epoch 17/25\n",
      "5948/5948 [==============================] - 1s 86us/step - loss: 10.6666 - mean_squared_error: 10.6666 - mean_absolute_error: 3.2242 - mean_absolute_percentage_error: 90.0295 - cosine_proximity: -1.0000\n",
      "Epoch 18/25\n",
      "5948/5948 [==============================] - 0s 77us/step - loss: 10.3169 - mean_squared_error: 10.3169 - mean_absolute_error: 3.1696 - mean_absolute_percentage_error: 88.4637 - cosine_proximity: -1.0000\n",
      "Epoch 19/25\n",
      "5948/5948 [==============================] - 0s 75us/step - loss: 9.9486 - mean_squared_error: 9.9486 - mean_absolute_error: 3.1109 - mean_absolute_percentage_error: 86.7830 - cosine_proximity: -1.0000\n",
      "Epoch 20/25\n",
      "5948/5948 [==============================] - 1s 91us/step - loss: 9.5625 - mean_squared_error: 9.5625 - mean_absolute_error: 3.0482 - mean_absolute_percentage_error: 84.9871 - cosine_proximity: -1.0000\n",
      "Epoch 21/25\n",
      "5948/5948 [==============================] - 0s 68us/step - loss: 9.1600 - mean_squared_error: 9.1600 - mean_absolute_error: 2.9815 - mean_absolute_percentage_error: 83.0803 - cosine_proximity: -1.0000\n",
      "Epoch 22/25\n",
      "5948/5948 [==============================] - 0s 63us/step - loss: 8.7428 - mean_squared_error: 8.7428 - mean_absolute_error: 2.9105 - mean_absolute_percentage_error: 81.0443 - cosine_proximity: -1.0000\n",
      "Epoch 23/25\n",
      "5948/5948 [==============================] - 0s 64us/step - loss: 8.3127 - mean_squared_error: 8.3127 - mean_absolute_error: 2.8358 - mean_absolute_percentage_error: 78.9094 - cosine_proximity: -1.0000\n",
      "Epoch 24/25\n",
      "5948/5948 [==============================] - 1s 102us/step - loss: 7.8721 - mean_squared_error: 7.8721 - mean_absolute_error: 2.7570 - mean_absolute_percentage_error: 76.6491 - cosine_proximity: -1.0000\n",
      "Epoch 25/25\n",
      "5948/5948 [==============================] - 0s 76us/step - loss: 7.4234 - mean_squared_error: 7.4234 - mean_absolute_error: 2.6744 - mean_absolute_percentage_error: 74.2881 - cosine_proximity: -1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "1487/1487 [==============================] - 0s 163us/step\n",
      "\n",
      "loss: 742.88%\n",
      "\n",
      "mean_squared_error: 742.88%\n",
      "\n",
      "mean_absolute_error: 267.98%\n",
      "\n",
      "mean_absolute_percentage_error: 7347.45%\n",
      "\n",
      "cosine_proximity: -100.00%\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "1859/1859 [==============================] - 0s 58us/step\n",
      "\n",
      "loss: 726.33%\n",
      "\n",
      "mean_squared_error: 726.33%\n",
      "\n",
      "mean_absolute_error: 264.92%\n",
      "\n",
      "mean_absolute_percentage_error: 7325.79%\n",
      "\n",
      "cosine_proximity: -100.00%\n"
     ]
    }
   ],
   "source": [
    "#Regression Model:\n",
    "#1 Layer: 76 -> 12.78% and 15.28%\n",
    "#2 Layer: 76,1 -> 0.40% and 0.48%\n",
    "#3 Layers: 76, 32, 1 -> 0% and 65%\n",
    "#4 layers: 76,48,32,1 + adam +  -> 60.52% and 56.70%\n",
    "#4 Layers: 76,32,16,1 -> 61.33% and 57.18%\n",
    "#4 Layers: 76,32,8,1 -> 0%\n",
    "#4 layers: 76,48,8,1 -> 0%\n",
    "#6 layers: 76,48,32,16,8,1 -> 20% and 0%\n",
    "\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "#parameters = number of neurons, initialization method, activation function\n",
    "model.add(Dense(76, input_dim=76, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(48, kernel_initializer='normal',activation = 'relu'))\n",
    "model.add(Dense(32, kernel_initializer='normal',activation = 'relu'))\n",
    "model.add(Dense(16, kernel_initializer='normal',activation = 'relu'))\n",
    "#model.add(Dense(8, kernel_initializer='normal',activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "# Compile model\n",
    "#opt = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae', 'mape', 'cosine'])\n",
    "history = model.fit(np.asarray(x_tra), y_tra, epochs=25)\n",
    "\n",
    "pyplot.plot(history.history['mean_squared_error'])\n",
    "pyplot.plot(history.history['mean_absolute_error'])\n",
    "pyplot.plot(history.history['mean_absolute_percentage_error'])\n",
    "pyplot.plot(history.history['cosine_proximity'])\n",
    "pyplot.show()\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "scores = model.evaluate(np.asarray(x_dev),y_dev)\n",
    "for i in range(len(scores)):\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[i], scores[i]*100))\n",
    "\n",
    "print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "scores = model.evaluate(np.asarray(x_test),y_test)\n",
    "for i in range(len(scores)):\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[i], scores[i]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.callbacks.History object at 0x000002560524C128>\n"
     ]
    }
   ],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=76, init = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(16, init = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(8, init = 'uniform', activation = 'softmax'))\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=25, batch_size=32)\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "scores = model.evaluate(x_train,y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "y_devpred = model.predict(x_dev)\n",
    "print(\"--------------------------------------------\")\n",
    "print(mean_squared_error(y_dev,y_devpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Softmax\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape = (x_train.shape)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "y_devpred = model.predict(x_dev)\n",
    "print(\"--------------------------------------------\")\n",
    "print(mean_squared_error(y_dev,y_devpred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
